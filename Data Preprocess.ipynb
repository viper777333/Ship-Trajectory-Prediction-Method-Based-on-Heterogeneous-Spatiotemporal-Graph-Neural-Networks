{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f2230ba0-bada-49f7-9eb0-9718eec856c2",
   "metadata": {},
   "source": [
    "<span style = 'color:red;font-size:25px'>提取相同的MMSI数据"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b619a4c0-828d-4db3-be82-7898dedeeee5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "# 定义输入文件路径\n",
    "input_files = [\n",
    "                             # 数据地址\n",
    "]\n",
    "\n",
    "# 定义输出文件夹路径\n",
    "output_folder = r\"\"\n",
    "\n",
    "# 确保输出文件夹存在\n",
    "os.makedirs(output_folder, exist_ok=True)\n",
    "\n",
    "# 初始化存储所有 MMSI 数据的字典\n",
    "mmsi_data = {}\n",
    "\n",
    "# 遍历每个输入文件并按 MMSI 号分类\n",
    "for file in input_files:\n",
    "    try:\n",
    "        # 读取 CSV 文件\n",
    "        df = pd.read_csv(file)\n",
    "        if 'MMSI' not in df.columns:\n",
    "            print(f\"文件 {file} 不包含 MMSI 列，跳过...\")\n",
    "            continue  # 如果文件没有 MMSI 列则跳过\n",
    "\n",
    "        # 按 MMSI 分组并保存到字典中\n",
    "        for mmsi, group in df.groupby('MMSI'):\n",
    "            if mmsi not in mmsi_data:\n",
    "                mmsi_data[mmsi] = group\n",
    "            else:\n",
    "                mmsi_data[mmsi] = pd.concat([mmsi_data[mmsi], group], ignore_index=True)\n",
    "    except Exception as e:\n",
    "        print(f\"处理文件 {file} 时出错: {e}\")\n",
    "\n",
    "# 保存每个 MMSI 的数据到单独的 CSV 文件\n",
    "for mmsi, data in mmsi_data.items():\n",
    "    output_file = os.path.join(output_folder, f\"{mmsi}.csv\")\n",
    "    data.to_csv(output_file, index=False)\n",
    "\n",
    "print(\"数据提取和保存完成！\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ac16931-d34a-4f2b-beac-09731e9b64a0",
   "metadata": {},
   "source": [
    "<span style = 'color:red;font-size:25px'>去除异常值点"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b5d68e6-aed3-4449-85d1-b3db3b0e0464",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "# 指定文件夹路径\n",
    "folder_path = r\"  \"\n",
    "\n",
    "# 获取文件夹中的所有CSV文件\n",
    "csv_files = [file for file in os.listdir(folder_path) if file.endswith(\".csv\")]\n",
    "\n",
    "# 遍历每个CSV文件\n",
    "for csv_file in csv_files:\n",
    "    # 构建完整的文件路径\n",
    "    file_path = os.path.join(folder_path, csv_file)\n",
    "    \n",
    "    # 读取CSV文件\n",
    "    data = pd.read_csv(file_path)\n",
    "    \n",
    "    # 转换 '# Timestamp' 列为 datetime\n",
    "    if 'BaseDateTime' in data.columns:\n",
    "        data['BaseDateTime'] = pd.to_datetime(data['BaseDateTime'], format='%Y-%m-%dT%H:%M:%S', errors='coerce')\n",
    "    \n",
    "        # 按时间戳升序排序\n",
    "        data.sort_values('BaseDateTime', inplace=True)\n",
    "    \n",
    "    # 删除 'SOG' 列中小于0.5或大于30的行\n",
    "    data = data[(data['SOG'] >= 0.5) & (data['SOG'] <= 30)]\n",
    "    \n",
    "    # 删除 'COG' 列中小于0或大于360的行\n",
    "    data = data[(data['COG'] >= 0) & (data['COG'] <= 360)]\n",
    "    \n",
    "    # 删除 'Latitude' 列中小于-90或大于90的行\n",
    "    data = data[(data['LAT'] >= -90) & (data['LAT'] <= 90)]\n",
    "    \n",
    "    # 删除 'Longitude' 列中小于0或大于180的行\n",
    "    data = data[(data['LON'] >= 0) & (data['LON'] <= 180)]\n",
    "    \n",
    "    # 删除 'Heading' 列中小于0或大于180的行\n",
    "    data = data[(data['Heading'] >= 0) & (data['Heading'] <= 180)]\n",
    "    \n",
    "    # 删除 \"ROT\" 列（如果需要删除时取消注释下面一行）\n",
    "    # data.drop('ROT', axis=1, inplace=True)\n",
    "    \n",
    "    # 覆盖原CSV文件\n",
    "    data.to_csv(file_path, index=False)\n",
    "\n",
    "print(\"数据处理完成。\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a83975d-6c13-4c86-bb05-b43c8985d2d2",
   "metadata": {},
   "source": [
    "<span style = 'color:red;font-size:25px'>填充有效的静态数据"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38698234-26de-43c4-a399-5c1b425a7a47",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "# 定义目标列\n",
    "target_columns = ['VesselType', 'Width', 'Length', 'Draft']\n",
    "\n",
    "# 获取目标文件夹中的所有CSV文件\n",
    "folder_path = r\"   \"\n",
    "csv_files = [file for file in os.listdir(folder_path) if file.endswith('.csv')]\n",
    "\n",
    "# 遍历文件夹中的每个CSV文件\n",
    "for csv_file in csv_files:\n",
    "    file_path = os.path.join(folder_path, csv_file)\n",
    "    \n",
    "    # 读取CSV文件\n",
    "    df = pd.read_csv(file_path)\n",
    "    \n",
    "    # 存储每列的有效值\n",
    "    valid_values = {}\n",
    "\n",
    "    # 遍历每个目标列，查找第一个有效值\n",
    "    for column in target_columns:\n",
    "        if column in df.columns:\n",
    "            # 提取有效值（去除空值、'undefined'、'Undefined'）\n",
    "            valid_value = df[column][(df[column].notna()) & ~(df[column].isin(['undefined', 'Undefined', '']))]\n",
    "            \n",
    "            # 直接取第一个有效值\n",
    "            if not valid_value.empty:\n",
    "                valid_values[column] = valid_value.iloc[0]\n",
    "            else:\n",
    "                valid_values[column] = None  # 如果没有有效值，设置为None\n",
    "\n",
    "    # 用提取的有效值填充目标列\n",
    "    for column in target_columns:\n",
    "        if column in valid_values and valid_values[column] is not None:\n",
    "            # 用提取到的有效值填充整列\n",
    "            df[column] = valid_values[column]\n",
    "    \n",
    "    # 保存修改后的CSV文件\n",
    "    df.to_csv(file_path, index=False)\n",
    "\n",
    "    print(f\"目标列已成功填充并保存到 {file_path}\")\n",
    "\n",
    "print(\"所有CSV文件处理完成。\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0d62b23-415f-440f-97ce-2edeeb45236f",
   "metadata": {},
   "source": [
    "<span style = 'color:red;font-size:25px'>转换时间戳类型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad35ff68-7a59-4f5c-93ed-2784509e6e9c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "# 指定目标文件夹路径\n",
    "folder_path = \"  \"\n",
    "\n",
    "# 获取目标文件夹中的所有CSV文件\n",
    "csv_files = [file for file in os.listdir(folder_path) if file.endswith(\".csv\")]\n",
    "\n",
    "# 遍历所有CSV文件\n",
    "for csv_file in csv_files:\n",
    "    file_path = os.path.join(folder_path, csv_file)\n",
    "    \n",
    "    # 读取CSV文件\n",
    "    data = pd.read_csv(file_path)\n",
    "    \n",
    "    # 检查时间戳列是否存在\n",
    "    if 'BaseDateTime' in data.columns:\n",
    "        # 尝试将时间戳列转换为datetime格式，遇到无法转换的就保留原格式\n",
    "        try:\n",
    "            data['BaseDateTime'] = pd.to_datetime(data['BaseDateTime'], format='%Y/%m/%d %H:%M:%S', errors='raise')\n",
    "        except ValueError:\n",
    "            # 如果发生异常，说明时间戳格式不符合要求，需要进行修改\n",
    "            data['BaseDateTime'] = pd.to_datetime(data['BaseDateTime'], errors='coerce')\n",
    "            # 将时间戳列格式化为目标格式\n",
    "            data['BaseDateTime'] = data['BaseDateTime'].dt.strftime('%Y/%m/%d %H:%M:%S')\n",
    "        \n",
    "        # 保存修改后的CSV文件\n",
    "        data.to_csv(file_path, index=False)\n",
    "\n",
    "print(\"CSV文件中的时间戳已根据需要进行更改。\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e09d0696-a45d-46b5-8af5-cec872570742",
   "metadata": {},
   "source": [
    "<span style = 'color:red;font-size:25px'>分割数据"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b031ad4c-309b-4e9f-9534-2c49a8f36e2c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "# 指定目标文件夹路径\n",
    "folder_path = \"  \"\n",
    "\n",
    "# 获取目标文件夹中的所有CSV文件\n",
    "csv_files = [file for file in os.listdir(folder_path) if file.endswith(\".csv\")]\n",
    "\n",
    "# 遍历所有CSV文件\n",
    "for csv_file in csv_files:\n",
    "    file_path = os.path.join(folder_path, csv_file)\n",
    "    \n",
    "    # 读取CSV文件\n",
    "    data = pd.read_csv(file_path)\n",
    "    \n",
    "    # 初始化数据分段的起点\n",
    "    segment_start = 0\n",
    "    segments = []  # 用于存储分段数据\n",
    "\n",
    "    # 遍历数据，计算相邻时间差\n",
    "    for i in range(1, len(data)):\n",
    "        # 将时间戳转换为pandas的日期时间对象\n",
    "        timestamp1 = pd.to_datetime(data['BaseDateTime'].iloc[i - 1], format='%Y/%m/%d %H:%M:%S')\n",
    "        timestamp2 = pd.to_datetime(data['BaseDateTime'].iloc[i], format='%Y/%m/%d %H:%M:%S')\n",
    "        \n",
    "        # 如果转换失败，则跳过此行\n",
    "        if pd.isna(timestamp1) or pd.isna(timestamp2):\n",
    "            continue\n",
    "        \n",
    "        # 计算相邻两行的时间差，单位为小时\n",
    "        timestamp_diff = (timestamp2 - timestamp1).total_seconds() / 3600  # 结果是小时\n",
    "\n",
    "        # 如果相邻两值的时间差大于6小时，分割数据\n",
    "        if timestamp_diff > 6:\n",
    "            # 创建新的分段\n",
    "            segment_data = data.iloc[segment_start:i]\n",
    "            segments.append(segment_data)\n",
    "            segment_start = i\n",
    "\n",
    "    # 如果有分段数据\n",
    "    if len(segments) > 0:\n",
    "        # 保存分段数据到新的CSV文件\n",
    "        for idx, segment_data in enumerate(segments):\n",
    "            # 提取MMSI号作为文件名前缀\n",
    "            mmsi_number = data['MMSI'].iloc[0] if 'MMSI' in data.columns else 'Unknown'\n",
    "            segment_filename = f\"{mmsi_number}_{idx + 1}.csv\"\n",
    "            segment_path = os.path.join(folder_path, segment_filename)\n",
    "            segment_data.to_csv(segment_path, index=False)\n",
    "\n",
    "        # 从原文件中删除与保存数据中相同 `# Timestamp` 值的行\n",
    "        for segment in segments:\n",
    "            data = data[~data['BaseDateTime'].isin(segment['BaseDateTime'])]\n",
    "\n",
    "        # 保存原文件\n",
    "        data.to_csv(file_path, index=False)\n",
    "\n",
    "print(\"CSV文件中的数据已根据# Timestamp分段，分段已从原文件中删除，并删除与保存数据中相同 `# Timestamp` 值的行。\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a496f86-ebf6-422b-ba2c-091f85a0bb42",
   "metadata": {},
   "source": [
    "<span style = 'color:red;font-size:25px'>删除跳点以及重复时间戳的点"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f79d282f-1ff4-4767-babb-4f2cd3812688",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import math\n",
    "\n",
    "def get_distance(lon1, lat1, lon2, lat2):\n",
    "    \"\"\"\n",
    "    计算经纬度之间的距离，单位千米\n",
    "    :param lon1: A点的经度\n",
    "    :param lat1: A点的纬度\n",
    "    :param lon2: B点的经度\n",
    "    :param lat2: B点的纬度\n",
    "    :return: 距离（千米）\n",
    "    \"\"\"\n",
    "    EARTH_RADIUS = 6371  # 地球半径，单位千米\n",
    "    lon1, lat1, lon2, lat2 = map(math.radians, [float(lon1), float(lat1), float(lon2), float(lat2)])\n",
    "    d_lon = lon2 - lon1\n",
    "    d_lat = lat2 - lat1\n",
    "    a = math.sin(d_lat / 2) ** 2 + math.cos(lat1) * math.cos(lat2) * math.sin(d_lon / 2) ** 2\n",
    "    c = 2 * math.asin(math.sqrt(a))\n",
    "    distance = c * EARTH_RADIUS\n",
    "    return distance\n",
    "\n",
    "def clear_list(gjd_list=None, max_speed=56, time_interval=6):\n",
    "    \"\"\"\n",
    "    清理重复时间戳并删除跳点\n",
    "    删除时间戳重复的数据，保留时间戳第一次出现的数据\n",
    "    删除跳点：基于相邻轨迹点的距离与时间差，超出阈值则删除\n",
    "    :param time_interval: 时间分割阈值，相邻两点超过特定值，此处为6h，以此分割轨迹段\n",
    "    :param max_speed: 正常速度阈值，相邻两点平均速度超过设定值(此处为56Km/h)，删除异常速度的轨迹点\n",
    "    :param gjd_list: 轨迹点列表\n",
    "    :return: 清理后的索引列表\n",
    "    \"\"\"\n",
    "    if not gjd_list:\n",
    "        return '传入列表为空'\n",
    "\n",
    "    # 按照时间去重\n",
    "    timestamp_list = []\n",
    "    repeat_timestamp = []\n",
    "    for gjd_index, gjd in enumerate(gjd_list):\n",
    "        gjd_timestamp = gjd[0]  # 时间戳在第0列\n",
    "        if gjd_timestamp in timestamp_list:\n",
    "            repeat_timestamp.append(gjd_index)  # 记录重复的索引\n",
    "        else:\n",
    "            timestamp_list.append(gjd_timestamp)\n",
    "\n",
    "    # 只保留未重复的行\n",
    "    valid_indices = [i for i in range(len(gjd_list)) if i not in repeat_timestamp]\n",
    "\n",
    "    # 处理跳点\n",
    "    final_indices = []  # 存储最终有效的轨迹点索引\n",
    "    for idx in valid_indices:\n",
    "        if idx + 1 >= len(gjd_list):  # 如果是最后一个点\n",
    "            final_indices.append(idx)\n",
    "            break\n",
    "\n",
    "        current_point = gjd_list[idx]  # 当前轨迹点\n",
    "        next_point = gjd_list[idx + 1]  # 下一个轨迹点\n",
    "\n",
    "        # 解析当前点和下一点的数据\n",
    "        lon1, lat1, timestamp1 = float(current_point[2]), float(current_point[1]), pd.to_datetime(current_point[0])\n",
    "        lon2, lat2, timestamp2 = float(next_point[2]), float(next_point[1]), pd.to_datetime(next_point[0])\n",
    "\n",
    "        # 计算两点之间的距离和时间差\n",
    "        distance = get_distance(lon1, lat1, lon2, lat2)  # 单位千米\n",
    "        time_diff = (timestamp2 - timestamp1).total_seconds() / 3600  # 单位小时\n",
    "\n",
    "        # 判断是否为跳点\n",
    "        if time_diff == 0 or (distance / time_diff) > max_speed:\n",
    "            continue  # 跳过跳点\n",
    "        else:\n",
    "            final_indices.append(idx)\n",
    "\n",
    "    # 返回清理后的索引\n",
    "    return final_indices\n",
    "\n",
    "# 文件处理\n",
    "target_folder = r\"F:\\MMSI数据(加州)\"  # 文件夹路径\n",
    "\n",
    "# 获取文件夹中的所有CSV文件\n",
    "csv_files = [file for file in os.listdir(target_folder) if file.endswith(\".csv\")]\n",
    "\n",
    "for csv_file in csv_files:\n",
    "    file_path = os.path.join(target_folder, csv_file)\n",
    "    try:\n",
    "        # 读取CSV文件\n",
    "        data = pd.read_csv(file_path)\n",
    "        \n",
    "        # 提取时间戳、纬度和经度\n",
    "        gjd_list = data[['BaseDateTime', 'LAT', 'LON']].values.tolist()\n",
    "\n",
    "        # 清理数据并返回有效的行索引\n",
    "        valid_indices = clear_list(gjd_list)\n",
    "        \n",
    "        # 根据有效行索引提取原数据中的行\n",
    "        cleaned_data = data.iloc[valid_indices]\n",
    "\n",
    "        # 将清理后的数据覆盖保存到原文件中\n",
    "        cleaned_data.to_csv(file_path, index=False)\n",
    "        # print(f\"文件 {csv_file} 处理完成。\")\n",
    "    except Exception as e:\n",
    "        print(f\"文件 {csv_file} 处理出错：{e}\")\n",
    "\n",
    "print(\"所有文件处理完成。\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c8558ae-a31b-4800-901e-b3d7dbe7d258",
   "metadata": {},
   "source": [
    "<span style = 'color:red;font-size:25px'>提取经纬度不在特定范围的数据"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7dcf44b-5df2-460f-8d17-e3e96093231d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "# 1. 定义经纬度范围（将度、分、秒转换为十进制度数）\n",
    "\n",
    "def dms_to_decimal(degrees, minutes, seconds):\n",
    "    return degrees + minutes / 60 + seconds / 3600\n",
    "\n",
    "# 区域二的纬度范围\n",
    "lat_max = dms_to_decimal(27, 37, 19.793)    # 27°37′19.793″\n",
    "lat_min = dms_to_decimal(25, 2, 57.534)     # 25°2′57.534″\n",
    "\n",
    "# 区域二的经度范围\n",
    "lon_max = dms_to_decimal(80, 30, 8.39)      # 80°30′8.39″\n",
    "lon_min = dms_to_decimal(78, 16, 39.534)    # 78°16′39.534″\n",
    "\n",
    "# 2. 文件夹路径\n",
    "target_folder = r\"  \"\n",
    "\n",
    "# 3. 初始化一个列表，用于存储超出范围的文件名和详细信息\n",
    "outside_range_files = []\n",
    "\n",
    "# 4. 获取文件夹中的所有 CSV 文件\n",
    "csv_files = [file for file in os.listdir(target_folder) if file.endswith(\".csv\")]\n",
    "\n",
    "# 5. 遍历每个 CSV 文件\n",
    "for csv_file in csv_files:\n",
    "    file_path = os.path.join(target_folder, csv_file)\n",
    "    try:\n",
    "        # 读取 CSV 文件，只读取 'Latitude' 和 'Longitude' 列\n",
    "        data = pd.read_csv(file_path, usecols=['LAT', 'LON'])\n",
    "\n",
    "        # 确保 'Latitude' 和 'Longitude' 列为数值型，并去除缺失值\n",
    "        data = data.dropna(subset=['LAT', 'LON'])\n",
    "        data['LAT'] = pd.to_numeric(data['LAT'], errors='coerce')\n",
    "        data['LON'] = pd.to_numeric(data['LON'], errors='coerce')\n",
    "\n",
    "        # 重新去除可能的 NaN 值\n",
    "        data = data.dropna(subset=['LAT', 'LON'])\n",
    "\n",
    "        # 检查是否存在超出纬度范围的值\n",
    "        lat_out_of_range = data[(data['LAT'] < lat_min) | (data['LAT'] > lat_max)]\n",
    "        is_lat_out = not lat_out_of_range.empty\n",
    "\n",
    "        # 检查是否存在超出经度范围的值\n",
    "        lon_out_of_range = data[(data['LON'] < lon_min) | (data['LON'] > lon_max)]\n",
    "        is_lon_out = not lon_out_of_range.empty\n",
    "\n",
    "        # 根据情况记录文件名、超出范围的信息和具体值\n",
    "        if is_lat_out or is_lon_out:\n",
    "            status = \"\"\n",
    "            details = \"\"\n",
    "            max_display = 5  # 最多显示5个超出值\n",
    "            if is_lat_out and is_lon_out:\n",
    "                status = \"Latitude and Longitude out of range\"\n",
    "                lat_values = lat_out_of_range['LAT'].values[:max_display]\n",
    "                lon_values = lon_out_of_range['LON'].values[:max_display]\n",
    "                details += f\"Latitude out of range values (showing up to {max_display}):\\n{lat_values}\\n\"\n",
    "                details += f\"Longitude out of range values (showing up to {max_display}):\\n{lon_values}\\n\"\n",
    "            elif is_lat_out:\n",
    "                status = \"Latitude out of range\"\n",
    "                lat_values = lat_out_of_range['LAT'].values[:max_display]\n",
    "                details += f\"Latitude out of range values (showing up to {max_display}):\\n{lat_values}\\n\"\n",
    "            else:\n",
    "                status = \"Longitude out of range\"\n",
    "                lon_values = lon_out_of_range['LON'].values[:max_display]\n",
    "                details += f\"Longitude out of range values (showing up to {max_display}):\\n{lon_values}\\n\"\n",
    "            outside_range_files.append((csv_file, status, details))\n",
    "    except Exception as e:\n",
    "        print(f\"读取文件 {csv_file} 时出错：{e}\")\n",
    "\n",
    "# 6. 输出所有符合条件的 CSV 文件名、超出范围的情况和具体值\n",
    "if outside_range_files:\n",
    "    print(\"以下文件包含了超出指定经纬度范围的值：\")\n",
    "    for filename, status, details in outside_range_files:\n",
    "        print(f\"\\n文件名: {filename}\")\n",
    "        print(f\"超出范围情况: {status}\")\n",
    "        print(f\"具体超出范围的值:\\n{details}\")\n",
    "else:\n",
    "    print(\"未找到包含超出指定经纬度范围的值的文件。\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b54e514f-7ce1-4fbc-906a-eccf11cc3b58",
   "metadata": {},
   "source": [
    "# <span style = 'color:red;font-size:25px'>数据插值"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26a62659-2d0e-47a1-8744-97648f1b6b5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from scipy.interpolate import InterpolatedUnivariateSpline\n",
    "\n",
    "# 指定文件夹路径\n",
    "input_folder = r\"   \"\n",
    "\n",
    "# 获取所有CSV文件\n",
    "csv_files = [file for file in os.listdir(input_folder) if file.endswith(\".csv\")]\n",
    "\n",
    "for csv_file in csv_files:\n",
    "    # 构建完整的文件路径\n",
    "    file_path = os.path.join(input_folder, csv_file)\n",
    "    \n",
    "    # 读取CSV文件\n",
    "    data = pd.read_csv(file_path)\n",
    "    \n",
    "    # 去重数据\n",
    "    data = data.drop_duplicates(subset='BaseDateTime', keep='first')\n",
    "\n",
    "    # 对数据按照 '# Timestamp' 列进行排序\n",
    "    data = data.sort_values(by='BaseDateTime')\n",
    "    \n",
    "    # 将 '# Timestamp' 列转换为 datetime 对象\n",
    "    data['BaseDateTime'] = pd.to_datetime(data['BaseDateTime'], format='%Y/%m/%d %H:%M:%S')\n",
    "    \n",
    "    # 创建以1分钟为间隔的新时间戳范围\n",
    "    start_time = data['BaseDateTime'].min()\n",
    "    end_time = data['BaseDateTime'].max()\n",
    "    new_timestamps = pd.date_range(start=start_time, end=end_time, freq='1min')  # 修改为 '1min'\n",
    "    \n",
    "    # 将时间戳转换为数值型（以秒为单位）用于插值\n",
    "    data['Timestamp_numeric'] = data['BaseDateTime'].astype(np.int64) // 10**9\n",
    "    new_timestamps_numeric = new_timestamps.astype(np.int64) // 10**9\n",
    "\n",
    "    # 确保有足够的数据点进行插值\n",
    "    if len(data) < 2:\n",
    "        continue  # 数据点不足，跳过\n",
    "\n",
    "    # 处理 'Latitude' 和 'Longitude' 列的插值\n",
    "    try:\n",
    "        valid_data = data.dropna(subset=['LAT', 'LON'])\n",
    "        if len(valid_data) < 4:\n",
    "            continue  # 数据点不足以进行插值\n",
    "\n",
    "        # 在新时间戳上进行插值\n",
    "        new_latitudes = []\n",
    "        new_longitudes = []\n",
    "        \n",
    "        for i in range(len(new_timestamps_numeric)):\n",
    "            timestamp = new_timestamps_numeric[i]\n",
    "            \n",
    "            # 找到相邻的时间点\n",
    "            prev_idx = np.searchsorted(valid_data['Timestamp_numeric'], timestamp, side='right') - 1\n",
    "            next_idx = prev_idx + 1\n",
    "\n",
    "            # 确保有前后两个点\n",
    "            if prev_idx >= 0 and next_idx < len(valid_data):\n",
    "                prev_timestamp = valid_data.iloc[prev_idx]['Timestamp_numeric']\n",
    "                next_timestamp = valid_data.iloc[next_idx]['Timestamp_numeric']\n",
    "\n",
    "                time_diff = next_timestamp - prev_timestamp\n",
    "                if time_diff <= 120:  # 小于或等于2分钟，使用三次样条插值\n",
    "                    spline_lat = InterpolatedUnivariateSpline(valid_data['Timestamp_numeric'], valid_data['LAT'])\n",
    "                    spline_lon = InterpolatedUnivariateSpline(valid_data['Timestamp_numeric'], valid_data['LON'])\n",
    "                    new_latitudes.append(spline_lat(timestamp))\n",
    "                    new_longitudes.append(spline_lon(timestamp))\n",
    "                else:  # 大于2分钟，使用线性插值\n",
    "                    lat_interp = np.interp(timestamp, valid_data['Timestamp_numeric'], valid_data['LAT'])\n",
    "                    lon_interp = np.interp(timestamp, valid_data['Timestamp_numeric'], valid_data['LON'])\n",
    "                    new_latitudes.append(lat_interp)\n",
    "                    new_longitudes.append(lon_interp)\n",
    "            else:\n",
    "                new_latitudes.append(np.nan)\n",
    "                new_longitudes.append(np.nan)\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"文件 {csv_file} 的 'Latitude' 和 'Longitude' 插值出错：{e}\")\n",
    "        continue\n",
    "\n",
    "    # 创建新的 DataFrame，包含插值后的数据\n",
    "    interpolated_data = pd.DataFrame({\n",
    "        'BaseDateTime': new_timestamps,\n",
    "        'LAT': new_latitudes,\n",
    "        'LON': new_longitudes\n",
    "    })\n",
    "\n",
    "    # 保留 '# Timestamp' 列的格式（如 '2020/8/5  8:47:40'）\n",
    "    interpolated_data['BaseDateTime'] = interpolated_data['BaseDateTime'].apply(lambda x: f\"{x.year}/{x.month}/{x.day} {x.hour}:{x.minute}:{x.second}\")\n",
    "\n",
    "    # 将新时间戳转换为数值型，方便后续计算\n",
    "    interpolated_data['Timestamp_numeric'] = pd.to_datetime(interpolated_data['BaseDateTime'], format='%Y/%m/%d %H:%M:%S').astype(np.int64) // 10**9\n",
    "\n",
    "    # 添加固定列：'MMSI'、'Ship type'、'Width'、'Length'、'Draught'\n",
    "    fixed_columns = ['MMSI', 'VesselType', 'Width', 'Length', 'Draft']\n",
    "    for col in fixed_columns:\n",
    "        if col in data.columns:\n",
    "            value = data[col].iloc[0]  # 获取第一个值\n",
    "            interpolated_data[col] = value\n",
    "        else:\n",
    "            interpolated_data[col] = np.nan  # 若列不存在，填充 NaN\n",
    "\n",
    "    # 对 'SOG'、'COG'、'Heading' 列进行前后3-5个数据的平均值计算\n",
    "    for col in ['SOG', 'COG', 'Heading']:\n",
    "        if col in data.columns:\n",
    "            # 准备数据\n",
    "            data_col = data[['Timestamp_numeric', col]].dropna()\n",
    "            data_col = data_col.sort_values('Timestamp_numeric').reset_index(drop=True)\n",
    "            # 初始化列表存储平均值\n",
    "            averages = []\n",
    "            timestamps_numeric = interpolated_data['Timestamp_numeric'].values\n",
    "            for timestamp in timestamps_numeric:\n",
    "                # 找到当前插值时间点在原始数据中的位置\n",
    "                idx = np.searchsorted(data_col['Timestamp_numeric'], timestamp)\n",
    "                # 获取前后3-5个数据点\n",
    "                start_idx = max(0, idx - 5)\n",
    "                end_idx = min(len(data_col), idx + 5)\n",
    "                window_data = data_col.iloc[start_idx:end_idx][col]\n",
    "                if len(window_data) >= 1:\n",
    "                    avg_value = window_data.mean()\n",
    "                    averages.append(avg_value)\n",
    "                else:\n",
    "                    averages.append(np.nan)\n",
    "            interpolated_data[col] = averages\n",
    "        else:\n",
    "            interpolated_data[col] = np.nan  # 若列不存在，填充 NaN\n",
    "\n",
    "    # 删除辅助列\n",
    "    interpolated_data.drop(columns=['Timestamp_numeric'], inplace=True)\n",
    "\n",
    "    # 保存插值后的数据，覆盖原文件\n",
    "    interpolated_data.to_csv(file_path, index=False)\n",
    "    \n",
    "    print(f\"文件 {csv_file} 已处理并覆盖保存。\")\n",
    "\n",
    "print(\"所有文件处理完毕。\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "904ef663-f528-4178-bdad-648f52645f69",
   "metadata": {},
   "source": [
    "<span style = 'color:red;font-size:25px'>中值滤波"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57ea80f8-e9c1-44ac-8e67-ff05367d7647",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from scipy.signal import medfilt\n",
    "\n",
    "# 指定文件夹路径\n",
    "input_folder = r\"F:\\MMSI数据(加州)\"\n",
    "\n",
    "# 获取所有CSV文件\n",
    "csv_files = [file for file in os.listdir(input_folder) if file.endswith(\".csv\")]\n",
    "\n",
    "for csv_file in csv_files:\n",
    "    try:\n",
    "        # 构建完整的文件路径\n",
    "        file_path = os.path.join(input_folder, csv_file)\n",
    "        \n",
    "        # 读取CSV文件\n",
    "        data = pd.read_csv(file_path)\n",
    "        \n",
    "        # 确保数据按照 '# Timestamp' 列进行排序\n",
    "        data = data.sort_values(by='BaseDateTime').reset_index(drop=True)\n",
    "        \n",
    "        # 检查是否存在缺失值，删除包含缺失值的行\n",
    "        data = data.dropna(subset=['LAT', 'LON']).reset_index(drop=True)\n",
    "        \n",
    "        # 设置中值滤波的窗口大小，必须为奇数\n",
    "        window_size = 5  # 您可以根据需要调整窗口大小\n",
    "\n",
    "        # 检查数据长度是否足够应用滤波\n",
    "        if len(data) >= window_size:\n",
    "            # 从第三个数据点开始应用中值滤波\n",
    "            # 提取需要滤波的部分\n",
    "            data_to_filter = data[['LAT', 'LON']].copy()\n",
    "\n",
    "            # 应用中值滤波\n",
    "            # 注意：为了从第三个数据点开始滤波，我们需要在前面填充适当数量的值\n",
    "            # 在这种情况下，我们可以使用 padding='edge' 参数来处理边界效应\n",
    "            data_filtered_lat = medfilt(data_to_filter['LAT'], kernel_size=window_size)\n",
    "            data_filtered_lon = medfilt(data_to_filter['LON'], kernel_size=window_size)\n",
    "\n",
    "            # 将滤波后的数据替换原始数据，从第三个数据点开始\n",
    "            data.loc[2:, 'LAT'] = data_filtered_lat[2:]\n",
    "            data.loc[2:, 'LON'] = data_filtered_lon[2:]\n",
    "        else:\n",
    "            print(f\"文件 {csv_file} 数据点少于窗口大小，跳过滤波。\")\n",
    "        \n",
    "        # 保存处理后的数据，覆盖原文件\n",
    "        data.to_csv(file_path, index=False)\n",
    "        \n",
    "        print(f\"文件 {csv_file} 已处理并覆盖保存。\")\n",
    "    except Exception as e:\n",
    "        print(f\"处理文件 {csv_file} 时出错：{e}\")\n",
    "        continue\n",
    "\n",
    "print(\"所有文件处理完毕。\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
