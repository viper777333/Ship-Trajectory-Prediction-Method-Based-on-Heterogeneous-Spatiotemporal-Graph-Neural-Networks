{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2f7f95d-a538-42b9-b91e-96b6bf227844",
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "import math\n",
    "import torch\n",
    "from d2l import torch as d2l\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch_geometric.data as pyg_data\n",
    "from torch_geometric.data import Data\n",
    "from torch_geometric.data import HeteroData, Batch\n",
    "from torch_geometric.transforms import ToUndirected\n",
    "from torch_geometric.nn import GATConv, global_mean_pool, global_max_pool, global_add_pool\n",
    "from torch_geometric.nn import to_hetero\n",
    "import torch_geometric.transforms as T\n",
    "from torch.masked import MaskedTensor\n",
    "from functools import partial\n",
    "from IPython import display\n",
    "import torch.nn.utils.parametrizations as parametrizations  \n",
    "from torch.nn.utils import weight_norm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29e99b2b-fda7-454f-8c71-af7cbce78590",
   "metadata": {},
   "source": [
    "<span style = 'color:red; font-size:25px'>MSE//RMSE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b107c442-30ed-4e28-a8aa-3d04b7890c25",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_mse(actual_values, predicted_values):\n",
    "    squared_errors = [(actual - predicted) ** 2 for actual, predicted in zip(actual_values, predicted_values)]\n",
    "    mse = sum(squared_errors) \n",
    "    return mse"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "274d09cf-3588-4d69-ae0f-7d4197ee6695",
   "metadata": {},
   "source": [
    "<span style = 'color:red; font-size:25px'>ADE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "572b5da0-4b98-4683-b858-e44f386feb02",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_ade(predictions, ground_truth, mask):\n",
    "    \"\"\"\n",
    "    Compute the ADE value\n",
    "    param predictions: Predicted tensor with shape (batch_size, pred_time, num_nodes, feature_dim)\n",
    "    param ground_truth: Ground-truth tensor with shape (batch_size, pred_time, num_nodes, feature_dim)\n",
    "    param mask: Mask matrix with shape (batch_size, pred_time, num_nodes, feature_dim)\n",
    "    return: The ADE value and the number of valid nodes\n",
    "    \"\"\"\n",
    "    # 计算 L2 范数（欧氏距离）：每个时间步、每个节点上的预测误差\n",
    "    displacement_error = torch.sqrt(torch.sum((predictions - ground_truth) ** 2, dim=-1))  # (batch_size, pred_time, num_nodes)\n",
    "    \n",
    "    # 将 mask 的最后一个维度降维以匹配 displacement_error\n",
    "    mask_reduced = mask.any(dim=-1).float()  # (batch_size, pred_time, num_nodes)\n",
    "    \n",
    "    # 将填充部分的误差置为 0\n",
    "    masked_error = displacement_error * mask_reduced  # (batch_size, pred_time, num_nodes)\n",
    "    \n",
    "    # 累计误差总和和有效节点数\n",
    "    total_error = masked_error.sum()    # 总误差\n",
    "    valid_count = mask_reduced.sum()  # 有效节点总数\n",
    "    \n",
    "    return total_error"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9cca251-bbbd-4cfc-8aec-9832146c2e4e",
   "metadata": {},
   "source": [
    "<span style = 'color:red; font-size:18px'>Data extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbce680b-e5b7-499e-8a46-f127451d4f6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_data(file_path, start_row, chunk_size):\n",
    "    \"\"\"\n",
    "    Read data from a CSV file in chunks, keeping missing values as missing values.\n",
    "    \n",
    "    param chunk_size: number of rows per chunk\n",
    "    return: the loaded DataFrame\n",
    "    \"\"\"\n",
    "    try:\n",
    "        data = pd.read_csv(file_path, header=None, skiprows=start_row, nrows=chunk_size, low_memory=False)\n",
    "        if len(data) < chunk_size:\n",
    "            print(f\"Data read is smaller than the expected chunk size ({chunk_size}).\")\n",
    "            return pd.DataFrame() \n",
    "        return data.fillna(np.nan) \n",
    "    except pd.errors.EmptyDataError:\n",
    "        print(\"No more data to read. Exiting.\")\n",
    "        return pd.DataFrame()  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9eaef94-6263-4889-b555-a7d92fea2f5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def slice_data_generator(file_path, input_len, pred_len, batch_size):\n",
    "    \"\"\"\n",
    "    Read the CSV with a sliding window and generate batched data\n",
    "    \"\"\"\n",
    "    total_len = input_len + pred_len\n",
    "    chunk_size = total_len + batch_size - 1  # Total number of rows required for one batch\n",
    "    start_row = 2  # Start reading from the third row (index 2)\n",
    "\n",
    "    while True:\n",
    "        # Step 1: Read the current batch of data\n",
    "        data = preprocess_data(file_path, start_row=start_row, chunk_size=chunk_size)\n",
    "        if data.empty:\n",
    "            break \n",
    "\n",
    "        # Step 2: Select valid columns \n",
    "        data = data.iloc[:, 1:]\n",
    "        data_values = data.values\n",
    "\n",
    "        # Step 3: Use a sliding window to extract all windows from the current chunk\n",
    "        Data = []\n",
    "        for start_idx in range(0, len(data_values) - total_len + 1, 1):  # 步长为1\n",
    "            Data.append(data_values[start_idx: start_idx + total_len])\n",
    "\n",
    "        # Step 4: If no valid window can be extracted, stop\n",
    "        if not Data:\n",
    "            print(\"No valid data windows extracted. Exiting.\")\n",
    "            break\n",
    "\n",
    "        yield Data\n",
    "        \n",
    "        start_row += chunk_size // 2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7bf10e4-ab35-4dbc-8ab3-a669248d7616",
   "metadata": {},
   "source": [
    "<span style = 'color:red; font-size:18px'>Min–max normalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41088d7b-056b-4326-9ff0-aebafdbde9ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize_datat(out_Y, max_values, min_values):\n",
    "    \"\"\"\n",
    "    Apply min–max normalization to a tensor of shape (batch_size, time_steps, num_nodes, features).\n",
    "\n",
    "    param out_Y: shape (batch_size, time_steps, num_nodes, features)\n",
    "    param max_values: shape (features,)\n",
    "    param min_values: shape (features,)\n",
    "    return: \n",
    "        - normalized_data: normalized tensor with the same shape as out_Y\n",
    "    \"\"\"\n",
    "    # Ensure `max_values` and `min_values` are tensors.\n",
    "    max_values = torch.tensor(max_values, device=out_Y.device)\n",
    "    min_values = torch.tensor(min_values, device=out_Y.device)\n",
    "\n",
    "    # Check dimensions\n",
    "    if max_values.dim() != 1 or min_values.dim() != 1:\n",
    "        raise ValueError(\"max_values and min_values should be 1D tensors, giving the per-feature maxima and minima\")\n",
    "\n",
    "    if max_values.size(0) != out_Y.size(-1):\n",
    "        raise ValueError(\"max_values and min_values must match the feature dimension\")\n",
    "\n",
    "    # Apply normalization\n",
    "    normalized_data = (out_Y - min_values) / (max_values - min_values)\n",
    "\n",
    "    return normalized_data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a3de59f-b6c3-442b-9176-e7647ed10f6d",
   "metadata": {},
   "source": [
    "<span style = 'color:red; font-size:18px'>Inverse normalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbab187e-e4e2-40aa-aeae-bef9cb41ce74",
   "metadata": {},
   "outputs": [],
   "source": [
    "def denormalize_data(normalized_data, max_values, min_values):\n",
    "    \"\"\"\n",
    "    Apply inverse normalization along the last dimension of a 3D tensor.\n",
    "    \n",
    "    param normalized_data: normalized data of shape (batch_size, num_points, num_features)\n",
    "    param max_values: list of per-feature maximum values\n",
    "    param min_values: list of per-feature minimum values\n",
    "    return: inverse-normalized data with the same shape as the input\n",
    "    \"\"\"\n",
    "    normalized_data = np.array(normalized_data)\n",
    "    max_values = np.array(max_values)\n",
    "    min_values = np.array(min_values)\n",
    "    max_values = max_values[..., 0, 0]\n",
    "    min_values = min_values[..., 0, 0]\n",
    "    assert normalized_data.shape[-1] == max_values.shape[0], \n",
    "    \n",
    "    denormalized_data = normalized_data * (max_values - min_values) + min_values\n",
    "    \n",
    "    return denormalized_data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3dea4fb9-6ff1-46a4-a19a-17d5f730a56c",
   "metadata": {},
   "source": [
    "<span style = 'color:red;font-size:25px'>Compute the haversine distance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63274d85-2abf-4448-bd66-97f885c3ae8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def haversine_distances(points, radius=6371):\n",
    "    \"\"\"\n",
    "    Compute the haversine distance between all node pairs, and set the main diagonal to 1.\n",
    "    \n",
    "    Parameters:\n",
    "    points: tensor of shape (N, 2), each row is a point’s lat/lon [latitude, longitude]\n",
    "    radius: Earth radius in kilometers, default 6371\n",
    "    \n",
    "    Return:\n",
    "    distances: tensor of shape (N, N), pairwise great-circle distances in nautical miles\n",
    "    \"\"\"\n",
    "    points_rad = points * torch.pi / 180.0  # Shape: (N, 2)\n",
    "    latitudes = points_rad[:, 0].unsqueeze(1)  # Shape: (N, 1)\n",
    "    longitudes = points_rad[:, 1].unsqueeze(1)  # Shape: (N, 1)\n",
    "    dlat = latitudes - latitudes.T  \n",
    "    dlon = longitudes - longitudes.T  \n",
    "    a = (torch.sin(dlat / 2) ** 2 +\n",
    "         torch.cos(latitudes) * torch.cos(latitudes.T) * torch.sin(dlon / 2) ** 2)\n",
    "    c = 2 * torch.arcsin(torch.sqrt(a))\n",
    "    distances_km = radius * c  \n",
    "    distances_nmi = distances_km / 1.852 \n",
    "    distances_nmi.fill_diagonal_(1.0)\n",
    "    \n",
    "    return distances_nmi\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d0b7ac8-807c-4b66-8887-6d244260ea95",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_data(F_data1, input_len, pred_len, statics_features):\n",
    "    total_len = input_len + pred_len  # Total window length\n",
    "    batch_size = len(F_data1)\n",
    "    sample_shape = np.array(F_data1[0]).shape\n",
    "    num_nodes = sample_shape[1] // 5  \n",
    "    statics_list = np.array([statics_features[str(i)] for i in range(num_nodes)], dtype=float)\n",
    "    \n",
    "    # Initialize outputs\n",
    "    F_p_all = []\n",
    "    input_X_all = []\n",
    "    output_Y_all = []\n",
    "    S_all = []\n",
    "    static_result_all = []\n",
    "    input_x_all = []\n",
    "    Static_result_all = []\n",
    "    for i in range(batch_size):     # Extract data for each batch\n",
    "        F_data = np.array(F_data1[i])  # [T, N * 5]\n",
    "        F_data_reshaped = F_data.reshape(total_len, num_nodes, 5)\n",
    "\n",
    "        # Check which nodes are fully valid (no NaNs) over the entire window\n",
    "        valid_node_mask = ~np.isnan(F_data_reshaped).any(axis=(0, 2))\n",
    "        f_p = np.where(valid_node_mask)[0].tolist()\n",
    "        F_p_all.append(f_p)  # Valid nodes in the current batch\n",
    " \n",
    "        # Split inputs and outputs\n",
    "        in_x_reshaped = F_data_reshaped[:input_len]\n",
    "        out_y_reshaped = F_data_reshaped[input_len:] \n",
    "        valid_in_mask = ~np.isnan(in_x_reshaped).any(axis=2)\n",
    "        all_valid_t, all_valid_nodes = np.where(valid_in_mask)  # Find time-step and node indices for valid nodes\n",
    "        non_empty_features_all = in_x_reshaped[all_valid_t, all_valid_nodes, :]   # Extract indices for the current batch\n",
    "        sta_result_all_nodes = statics_list[all_valid_nodes]   \n",
    "\n",
    "        unique_t, counts = np.unique(all_valid_t, return_counts=True)  # Group data by time step\n",
    "        split_indices = np.split(np.arange(len(all_valid_t)), np.cumsum(counts[:-1]))  # Split by the number of valid nodes at each time step\n",
    "\n",
    "        # Restore the per-time-step data list\n",
    "        input_X = [non_empty_features_all[idx] for idx in split_indices]\n",
    "        S = [all_valid_nodes[idx] for idx in split_indices]\n",
    "        static_result = [sta_result_all_nodes[idx] for idx in split_indices]\n",
    "\n",
    "        input_X_all.append(input_X)\n",
    "        S_all.append(S)\n",
    "        static_result_all.append(static_result)\n",
    "\n",
    "        # Build the prediction output output_Y\n",
    "        if not f_p: \n",
    "            output_y = np.array([])\n",
    "            input_x0 = np.array([])\n",
    "            statics_list0 = np.array([])\n",
    "        else:\n",
    "            output_y = out_y_reshaped[:, f_p, :]  \n",
    "            input_x0 = in_x_reshaped[:, f_p, :]\n",
    "            Statics_list0 = statics_list[f_p, :]\n",
    "            statics_list0 = np.repeat(Statics_list0[np.newaxis, :, :], input_x0.shape[0], axis=0)\n",
    "        output_Y_all.append(output_y)\n",
    "        input_x_all.append(input_x0)\n",
    "        Static_result_all.append(statics_list0)\n",
    "        \n",
    "    return input_X_all, F_p_all, output_Y_all, S_all, static_result_all, input_x_all, Static_result_all"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58a1cea3-3503-40a6-bd72-7153766d220a",
   "metadata": {},
   "source": [
    "<span style = 'color:red;font-size:18px'>Extract static information (returned as a dictionary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d046f334-22a5-48d3-ade6-a13caf24324f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_mmsi_features(file_path):\n",
    "    \"\"\"\n",
    "    Extract MMSI entries and their feature values from the file, and replace MMSI keys with sequential indices.\n",
    "    \n",
    "    :param file_path: file path\n",
    "    :return: a dictionary with sequential indices (starting from 1) as keys and feature-value lists as values\n",
    "    \"\"\"\n",
    "    data = pd.read_csv(file_path, header=None)\n",
    "    mmsi_row = data.iloc[0]\n",
    "    feature_row = data.iloc[2]\n",
    "    mmsi_features = {}\n",
    "    \n",
    "    # Iterate over MMSI entries and their feature values\n",
    "    for col_idx, mmsi in enumerate(mmsi_row):\n",
    "        if pd.notna(mmsi):  \n",
    "            if mmsi not in mmsi_features:\n",
    "                mmsi_features[mmsi] = []\n",
    "            mmsi_features[mmsi].append(feature_row[col_idx])\n",
    "    \n",
    "    # Replace keys with sequential indices\n",
    "    indexed_features = {}\n",
    "    for index, (key, value) in enumerate(mmsi_features.items(), start=0):\n",
    "        indexed_features[str(index)] = value  \n",
    "    \n",
    "    return indexed_features"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6d98b14-94ec-4d1a-a157-a679007175ba",
   "metadata": {},
   "source": [
    "<span style = 'color:red;font-size:18px'>Convert one-hot encodings to indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06311923-e3ee-4535-a3d6-5469ba60856e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def one_hot_to_index(one_hot_str):\n",
    "    \"\"\"\n",
    "    Parse the one-hot encoded string into an index.\n",
    "    \"\"\"\n",
    "    one_hot_list = list(map(int, one_hot_str.split(',')))\n",
    "    return one_hot_list.index(1)\n",
    "\n",
    "def transform_data(data_dict):\n",
    "    \"\"\"\n",
    "    Convert the dictionary to a new format, replacing one-hot encodings with indices.\n",
    "    \"\"\"\n",
    "    transformed_data = {}\n",
    "    for key, value in data_dict.items():\n",
    "        index = one_hot_to_index(value[0]) \n",
    "        transformed_data[key] = [index] + value[1:]\n",
    "    return transformed_data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cff4c454-ef95-4cb3-accd-89c7bec46cb9",
   "metadata": {},
   "source": [
    "# <span style = 'color:red;font-size:18px'>Extract static feature data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2488d5d0-8ef8-4b42-92a9-366837553fa2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_sfeatures(S, features_dict):\n",
    "    \"\"\"\n",
    "    Extract features for the node IDs in list S, and cast feature values to float.\n",
    "    \n",
    "    :param S: a list where each element is an array containing node IDs\n",
    "    :param features_dict: a dict mapping node IDs to feature lists\n",
    "    :return: a list of node-feature lists at each time step, with features as floats\n",
    "    \"\"\"\n",
    "    result = []\n",
    "\n",
    "    for node_ids in S:\n",
    "        node_features = [\n",
    "            [float(value) for value in features_dict.get(str(node_id), [])] \n",
    "            for node_id in node_ids\n",
    "        ]\n",
    "        result.append(node_features)\n",
    "\n",
    "    return result"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92670b46-11e3-4060-a17c-445d179fd12d",
   "metadata": {},
   "source": [
    "<span style = 'color:red;font-size:18px'>Convert to one-hot encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "747605a2-c79e-4bed-a1b8-b72bde65ea0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_to_onehot(data, device, onehot_length = 20):\n",
    "    \"\"\"\n",
    "    Convert the first feature of an input 2D tensor to a one-hot vector and concatenate it with the remaining features.\n",
    "    \n",
    "    :param data: input 2D tensor of shape [N, F], where N is the number of nodes and F is the number of features\n",
    "    :param onehot_length: length of the one-hot vector\n",
    "    :return: output tensor of shape [N, onehot_length + F - 1]\n",
    "    \"\"\"\n",
    "    indices = data[:, 0].long()\n",
    "    \n",
    "    # Create the one-hot tensor\n",
    "    onehot = torch.zeros(data.size(0), onehot_length, dtype=torch.float32, device = device) \n",
    "    onehot.scatter_(1, indices.unsqueeze(1), 1)  \n",
    "    \n",
    "    return onehot"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd2d60c1-a3d9-447e-8f9e-8126841b2150",
   "metadata": {},
   "source": [
    "<span style = 'color:red;font-size:25px'>Heterogeneous Maritime Graph (HMG)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "356b5197-794b-4a02-b2f9-b125554e412b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Spatial_GATD(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, gat_heads):\n",
    "        super(Spatial_GATD, self).__init__()\n",
    "        self.gat1 = GATConv(input_dim, hidden_dim, heads=gat_heads, concat=True, add_self_loops=False)\n",
    "        self.gat2 = GATConv(hidden_dim * gat_heads, input_dim, heads=1, concat=True, add_self_loops=False)\n",
    "\n",
    "    def forward(self, x, edge_index, edge_attr):\n",
    "\n",
    "        x = self.gat1(x, edge_index, edge_attr = edge_attr) \n",
    "        x = F.elu(x)\n",
    "        x = self.gat2(x, edge_index, edge_attr = edge_attr)\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b1a411d-ef9d-4025-9788-86712370b4a1",
   "metadata": {},
   "source": [
    "<span style = 'color:red;font-size:18px'>Heterogeneous-graph computation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "747dca88-8cdd-47ce-a1da-e3114f456804",
   "metadata": {},
   "outputs": [],
   "source": [
    "class H_Model(torch.nn.Module):\n",
    "    def __init__(self, hidden_dimS, num_heads, embedding_dim2):\n",
    "        super(H_Model, self).__init__()\n",
    "        self.gatD = Spatial_GATD(hidden_dimS, hidden_dimS, num_heads)\n",
    "\n",
    "        metadataD = (\n",
    "                          ['DYA', 'STA'],  # Node types\n",
    "                          [\n",
    "                              ('DYA', 'DD', 'DYA'),    # Edge type DD: DYA → DYA\n",
    "                              ('DYA', 'DS', 'STA'),    # Edge type DS: DYA → STA\n",
    "                              ('STA', 'rev_DS', 'DYA'), # Edge type rev_DS: STA → DYA\n",
    "                              ('STA', 'SS', 'STA')    # Edge type SS: STA → STA\n",
    "                          ]\n",
    "                    )\n",
    "        \n",
    "        self.gatD = to_hetero(self.gatD, metadata=metadataD)\n",
    "\n",
    "    def forward(self, xd_dict, data_D):\n",
    "        edge_indexD = data_D.edge_index_dict\n",
    "        edge_attrD = data_D.edge_attr_dict\n",
    "        x_dictd = self.gatD(xd_dict, edge_indexD, edge_attrD)\n",
    "        \n",
    "        return x_dictd['DYA'], x_dictd['STA']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4faa475-b37c-41b8-9912-adee63ef2771",
   "metadata": {},
   "source": [
    "<span style = 'color:red;font-size:20px'>Build the heterogeneous-graph dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab0d55be-4b96-49f6-ac84-de923bf58cd9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def HeteroGraphBuilder_batch(dynamic_tensor, static_tensor, masks, device, distances):\n",
    "    \"\"\"\n",
    "    Build heterogeneous-graphdata from inputs of shape (batch_size, time_steps, num_nodes, feature_dim)\n",
    "    Dynamic and static nodes have the same count but different feature dimensions.\n",
    "    Also generate the corresponding batch vector\n",
    "    param dynamic_tensor: dynamic-node feature tensor (batch_size, time_steps, num_nodes, dynamic_feat_dim)\n",
    "    param static_tensor: static-node feature tensor (batch_size, time_steps, num_nodes, static_feat_dim)\n",
    "    param masks: mask tensor (batch_size, time_steps, num_nodes)，where 0 indicates nodes to be removed\n",
    "    param device: device\n",
    "    return: dataD (HeteroData), batch (Tensor)\n",
    "    \"\"\"    \n",
    "    B, T, N, F_d = dynamic_tensor.shape\n",
    "    _, _, _, F_s = static_tensor.shape\n",
    "    \n",
    "    dynamic_tensor = dynamic_tensor.to(device)\n",
    "    static_tensor = static_tensor.to(device)\n",
    "    masks = masks.to(device)\n",
    "    \n",
    "    total_subgraphs = B * T  # Total number of subgraphs\n",
    "    total_nodes = B * T * N\n",
    "\n",
    "    # Flatten global features for dynamic and static nodes\n",
    "    X_D_global = dynamic_tensor.reshape(total_subgraphs * N, F_d)     # shape(batch_size * time_steps * num_nodes, feature_dim)\n",
    "    X_S_global = static_tensor.reshape(total_subgraphs * N, F_s)      # shape(batch_size * time_steps * num_nodes, feature_dim)\n",
    "\n",
    "    # Generate the batch vector\n",
    "    batch_vector = torch.repeat_interleave(torch.arange(total_subgraphs, device=device), N)\n",
    "\n",
    "    # Node indices for a single subgraph\n",
    "    node_ids_local = torch.arange(N, dtype=torch.long, device=device)\n",
    "\n",
    "    # Edge indices for a single subgraph (same logic for dynamic and static nodes), single edges\n",
    "    if N > 1:\n",
    "        edges_local = torch.combinations(node_ids_local.cpu(), r=2, with_replacement=False).T.to(device)   # Fully connect nodes\n",
    "    else:\n",
    "        edges_local = torch.empty((2, 0), dtype=torch.long, device=device)\n",
    "    self_loops = torch.stack([node_ids_local, node_ids_local], dim=0)\n",
    "    edges_local = torch.cat([edges_local, self_loops], dim=1)  # (2, num_edges)\n",
    "    \n",
    "    # Compute the global offset\n",
    "    offsets = torch.arange(total_subgraphs, device=device) * N   \n",
    "    \n",
    "    # Generate all possible edges (including self-loops)\n",
    "    cross_edges_local = torch.cartesian_prod(node_ids_local, node_ids_local).T.to(device)  # (2, N*N)\n",
    "    \n",
    "    # Remove self-loop edges\n",
    "    mask = cross_edges_local[0] != cross_edges_local[1]\n",
    "    cross_edges_local = cross_edges_local[:, mask]  # (2, N*(N-1))\n",
    "    offsets_flattened = offsets.flatten()\n",
    "    \n",
    "    # Expand local edge indices to all subgraphs\n",
    "    cross_edges_expanded = (\n",
    "        cross_edges_local.unsqueeze(1) + offsets_flattened.unsqueeze(0).unsqueeze(-1)\n",
    "    ).reshape(2, -1) \n",
    "    edges_expanded = (edges_local.unsqueeze(1) + offsets.unsqueeze(0).unsqueeze(-1)).reshape(2, -1)   # Generate global edges (same node across subgraphs)\n",
    "    \n",
    "    # Construct the HeteroData object\n",
    "    dataD = HeteroData()\n",
    "    \n",
    "    dataD['DYA'].x = X_D_global\n",
    "    dataD['DYA', 'DD', 'DYA'].edge_index = edges_expanded\n",
    "    dataD['STA'].x = X_S_global\n",
    "    dataD['DYA', 'DS', 'STA'].edge_index = cross_edges_expanded\n",
    "    edgeD_DD = dataD['DYA', 'DD', 'DYA'].edge_index \n",
    "    edgeD_DS = dataD['DYA', 'DS', 'STA'].edge_index  \n",
    "\n",
    "    # Locate padded nodes (values equal to 0); strictly keep the original node IDs and do not change them.\n",
    "    flattened_tensor = masks.flatten()\n",
    "    zero_indices = (flattened_tensor == 0).nonzero(as_tuple=False).squeeze()\n",
    "    Mask = torch.ones(batch_vector.size(0), dtype=torch.bool, device=device)\n",
    "    Mask[zero_indices] = False  \n",
    "    batch = batch_vector[Mask]\n",
    "\n",
    "    # Filter edges for dynamic nodes\n",
    "    maskD_DD = ~(\n",
    "        torch.isin(edgeD_DD[0], zero_indices) |\n",
    "        torch.isin(edgeD_DD[1], zero_indices)\n",
    "    )\n",
    "    maskD_DS = ~(\n",
    "        torch.isin(edgeD_DS[0], zero_indices) |\n",
    "        torch.isin(edgeD_DS[1], zero_indices)\n",
    "    )\n",
    "   \n",
    "    # Remove unused nodes and reconnect edges\n",
    "    edgeD_DD = edgeD_DD[:, maskD_DD]\n",
    "    edgeD_DS = edgeD_DS[:, maskD_DS]\n",
    "  \n",
    "    # Remap edge indices\n",
    "    dataD['DYA', 'DD', 'DYA'].edge_index = edgeD_DD\n",
    "    dataD['DYA', 'DS', 'STA'].edge_index = edgeD_DS\n",
    "    dataD['STA', 'SS', 'STA'].edge_index = edgeD_DD\n",
    "\n",
    "    # Assign edge attributes\n",
    "    adj_DD = distances[edgeD_DD[0], edgeD_DD[1]]\n",
    "    adj_DS = distances[edgeD_DS[0], edgeD_DS[1]]\n",
    "    dataD['DYA', 'DD', 'DYA'].edge_attr = adj_DD\n",
    "    dataD['DYA', 'DS', 'STA'].edge_attr = adj_DS\n",
    "    dataD['STA', 'SS', 'STA'].edge_attr = adj_DD\n",
    "    \n",
    "    # Convert to an undirected graph\n",
    "    dataD = ToUndirected()(dataD)\n",
    "\n",
    "    return dataD, batch"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "302d7b93-a19d-48c9-814e-e41a936fecad",
   "metadata": {},
   "source": [
    "<span style = 'color:red;font-size:18px'>Get data suitable for iTransformer input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c1925c5-2b90-4950-ad9c-cef7bf172d6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_tensor_and_extract_features(tensor, batch_size, input_len, masks, device):\n",
    "    \"\"\"\n",
    "    Split the input tensor into multiple time steps based on the batch values, and extract per-time-step features using indices in F.\n",
    "    :param tensor: input tensor of shape (batch_size, time_steps, total_nodes, feature_dim)\n",
    "    :param masks: tensor of shape (batch_size, time_steps, total_nodes)\n",
    "    :param batch: batch indices of shape (total_nodes,)\n",
    "    :param F: index list of prediction nodes in the original full data, shape (batch_size, node_id)\n",
    "    :param S: per-time-step node indices in the original data, shape (batch_size, time_steps, node_id)\n",
    "    :return: a 3D tensor of shape (time_steps, num_nodes, feature_dim)\n",
    "    \"\"\"\n",
    "    # 1) Extract the real data for num_nodes per batch\n",
    "    result_list = []\n",
    "    for b in range(batch_size):\n",
    "        mask_b = masks[b] \n",
    "        valid_nodes = mask_b.any(dim=0)  \n",
    "        valid_node_indices = torch.where(valid_nodes)[0]  # Valid node indices\n",
    "        tensor_valid = tensor[b, :, valid_node_indices, :]  # shape: (T, num_valid_nodes, F)\n",
    "        result_list.append(tensor_valid)\n",
    "    \n",
    "    # 2) Concatenate results and reshape to the target shape\n",
    "    final_result = torch.cat(result_list, dim=1).permute(1, 0, 2).to(device) # shape: (B * N, T, F)\n",
    "    final_result = final_result.permute(0, 2, 1) # shape: (B * N, F, T)\n",
    "\n",
    "    return final_result.reshape(final_result.shape[0], final_result.shape[1], 1, final_result.shape[2])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9e0df19-78b2-496e-8fc9-cd8553562c1d",
   "metadata": {},
   "source": [
    "<span style = 'color:red;font-size:25px'>LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50a4ff94-cc7b-42d2-987e-4ce0e921053b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LSTMNetwork(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, num_layers, bidirectional=False, dropout=0.0):\n",
    "        super(LSTMNetwork, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_layers = num_layers\n",
    "        self.bidirectional = bidirectional\n",
    "\n",
    "        self.lstm = nn.LSTM(input_size=input_size,\n",
    "                            hidden_size=hidden_size,\n",
    "                            num_layers=num_layers,\n",
    "                            batch_first=True,\n",
    "                            bidirectional=bidirectional,\n",
    "                            dropout=dropout if num_layers > 1 else 0.0)\n",
    "    \n",
    "    def forward(self, x):\n",
    "\n",
    "        output, (h_n, c_n) = self.lstm(x)\n",
    "        \n",
    "        return output, h_n, c_n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41137909-4f34-4e2d-8109-47b11f8c3c76",
   "metadata": {},
   "source": [
    "<span style = 'color:Red;font-size:25px'>Dual-Axis Attention (DAA)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce1f74d8-06fa-42e8-8522-4f38beebb45b",
   "metadata": {},
   "source": [
    "<span style = 'color:blue;font-size:20px'>Average pooling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61908f5a-bff5-4af2-9abf-0b43abedc697",
   "metadata": {},
   "outputs": [],
   "source": [
    "def masked_average_pooling(x, mask, pool_dim, device):\n",
    "    \"\"\"\n",
    "    x: input tensor of shape (batch_size, feature_dim, num_nodes, time_steps)\n",
    "    mask: mask tensor of shape (batch_size, time_steps, num_nodes)\n",
    "    pool_dim: pooling dimension, 0 -> num_nodes, 1 -> time_steps, 2 -> feature_dim\n",
    "    return: average-pooled tensor\n",
    "    \"\"\"\n",
    "    x = x.to(device)\n",
    "    mask = mask.to(device)\n",
    "\n",
    "    mask = mask.unsqueeze(1)  # reshape (B, 1, T, N)\n",
    "    mask = mask.permute(0, 1, 3, 2)  \n",
    "    mask = mask.type_as(x)\n",
    "    x_masked = x * mask \n",
    "    \n",
    "    # Compute the number of valid elements\n",
    "    mask_sum = mask.sum(dim=2)  \n",
    "    mask_sum = mask_sum.clamp(min=1e-6)  \n",
    "    \n",
    "    # Pool along different dimensions\n",
    "    if pool_dim == 2:  # Pool along the nodes dimension\n",
    "        x_sum = x_masked.sum(dim=2)  \n",
    "        x_avg = x_sum / mask_sum\n",
    "        \n",
    "    elif pool_dim == 3:  # Pool along the time_steps dimension\n",
    "        x_sum = x_masked.sum(dim=3) \n",
    "        mask_sum = mask.sum(dim=3)  \n",
    "        x_avg = x_sum / mask_sum.clamp(min=1e-6)\n",
    "        \n",
    "    elif pool_dim == 1:  # Pool along the feature_dim dimension\n",
    "        mask_expanded = mask.expand(-1, x.size(1), -1, -1)  \n",
    "        mask_sum = mask_expanded.sum(dim=1)\n",
    "        mask_sum = mask_sum.clamp(min=1e-6)  \n",
    "        x_sum = x_masked.sum(dim=1)  \n",
    "        x_avg = x_sum / mask_sum\n",
    "    return x_avg  # shape (batch_size, feature_dim, num_nodes, time_steps)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6031276b-96c4-4060-9033-474242169279",
   "metadata": {},
   "source": [
    "<span style = 'color:blue;font-size:20px'>Max pooling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6bad00ff-2b33-44b0-91f4-fdb8b7336aa7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def masked_max_pooling(x, mask, pool_dim, device):\n",
    "    \"\"\"\n",
    "    x: input tensor of shape (batch_size, feature_dim, num_nodes, time_steps)\n",
    "    mask: mask tensor of shape (batch_size, time_steps, num_nodes)\n",
    "    pool_dim: pooling dimension, 0 -> num_nodes, 1 -> time_steps, 2 -> feature_dim\n",
    "    return: max-pooled tensor\n",
    "    \"\"\"\n",
    "    mask = mask.unsqueeze(1)  \n",
    "    mask = mask.permute(0, 1, 3, 2)  \n",
    "    mask = mask.to(device)\n",
    "    x = x.to(device)\n",
    "    x_masked = x.masked_fill(mask == 0, float('-inf'))\n",
    "    \n",
    "    # Pool along different dimensions\n",
    "    if pool_dim == 2:  # Pool along the nodes dimension\n",
    "        x_max, _ = x_masked.max(dim=2)  \n",
    "        x_max = x_max.masked_fill(mask.sum(dim=2) == 0, 0)\n",
    "\n",
    "    elif pool_dim == 3:  # Pool along the time_steps dimension\n",
    "        x_max, _ = x_masked.max(dim=3) \n",
    "        x_max = x_max.masked_fill(mask.sum(dim=3) == 0, 0)\n",
    "        \n",
    "    elif pool_dim == 1:  # Pool along the feature_dim dimension\n",
    "        x_max, _ = x_masked.max(dim=1) \n",
    "        x_max = x_max.masked_fill(mask.sum(dim=1) == 0, 0)\n",
    "    return x_max  # shape (batch_size, feature_dim, num_nodes, time_steps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ab66767-34ce-4e40-af99-0a2a7b548e54",
   "metadata": {},
   "outputs": [],
   "source": [
    "class AttentionModule(nn.Module):\n",
    "    def __init__(self, kernel_size, pool_dim1, pool_dim2, input_channels = 1):\n",
    "        super(AttentionModule, self).__init__()\n",
    "        self.pool_dim1 = pool_dim1\n",
    "        self.pool_dim2 = pool_dim2\n",
    "\n",
    "        # Nodes–time interaction\n",
    "        self.nt_conv = nn.Sequential(\n",
    "            nn.Conv2d(2, 1, kernel_size=kernel_size, padding = (kernel_size - 1) // 2),\n",
    "            nn.BatchNorm2d(1),\n",
    "            nn.ReLU(),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "        \n",
    "        # Feature–nodes interaction\n",
    "        self.fn_conv = nn.Sequential(\n",
    "            nn.Conv2d(2 * input_channels, input_channels, kernel_size=kernel_size, padding = (kernel_size - 1) // 2),\n",
    "            nn.BatchNorm2d(1),\n",
    "            nn.ReLU(),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "        \n",
    "    def forward(self, x, mask, device):\n",
    "        \"\"\"\n",
    "        x: input tensor of shape (batch_size, feature_dim, num_nodes, time_steps)\n",
    "        mask: mask tensor of shape (batch_size, time_steps, num_nodes)\n",
    "        pool_dim: pooling dimension, 0 -> num_nodes, 1 -> time_steps, 2 -> feature_dim\n",
    "        return: max-pooled tensor\n",
    "        \"\"\"\n",
    "        # nodes - time_steps\n",
    "        avg_nt = masked_average_pooling(x, mask, self.pool_dim1, device)\n",
    "        max_nt = masked_max_pooling(x, mask, self.pool_dim1, device)\n",
    "        avg_nt = avg_nt.unsqueeze(1)\n",
    "        max_nt = max_nt.unsqueeze(1)\n",
    "        am_nt = torch.cat((avg_nt, max_nt), dim = 1)\n",
    "        att_nt = self.nt_conv(am_nt) # shape(B, 1, N, T)\n",
    "        x_ant = att_nt * x\n",
    "        \n",
    "        # feature - nodes\n",
    "        avg_fn = masked_average_pooling(x, mask, self.pool_dim2, device)\n",
    "        max_fn = masked_max_pooling(x, mask, self.pool_dim2, device)\n",
    "        avg_fn = avg_fn.unsqueeze(1)\n",
    "        max_fn= max_fn.unsqueeze(1)\n",
    "        am_fn = torch.cat((avg_fn, max_fn), dim = 1)\n",
    "        att_fn = self.fn_conv(am_fn) # shape(B, 1, F, N)\n",
    "        \n",
    "        data_pfn = x.permute(0, 3, 1, 2)\n",
    "        weighted_dfn = att_fn * data_pfn\n",
    "        x_afn = weighted_dfn.permute(0, 2, 3, 1)\n",
    "        \n",
    "        return x_ant + x_afn  # shape(B, F, N, T)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38be0b4f-075c-47da-b506-3cce87470b6f",
   "metadata": {},
   "source": [
    "<span style = 'color:red;font-size:25px'>iTransformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14049123-b405-4ff5-b071-1cd271f64ddd",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GeLU(nn.Module):\n",
    "    def forward(self, input_tensor):\n",
    "        return 0.5 * input_tensor * (1 + torch.tanh(math.sqrt(2 / math.pi) * (input_tensor + 0.044715 * torch.pow(input_tensor, 3))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b300bc6-558d-4846-8443-16ad15240df2",
   "metadata": {},
   "outputs": [],
   "source": [
    "class AddNorm(nn.Module):\n",
    "    \"Apply layer normalization after the residual connection.\"\n",
    "    def __init__(self, normalized_shape, dropout, **kwargs):\n",
    "        super(AddNorm, self).__init__(**kwargs)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.ln = nn.LayerNorm(normalized_shape)\n",
    "\n",
    "    def forward(self, X, Y):\n",
    "        return self.ln(self.dropout(Y) + X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d89eb2e5-8df4-43a2-aaae-58cd92348673",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PositionWiseFFN(nn.Module):\n",
    "    \"Position-wise feed-forward network.\"\n",
    "    def __init__(self, ffn_num_input, ffn_num_hiddens, ffn_num_outputs,\n",
    "                 **kwargs):\n",
    "        super(PositionWiseFFN, self).__init__(**kwargs)\n",
    "        self.dense1 = nn.Linear(ffn_num_input, ffn_num_hiddens)\n",
    "        self.gelu = GeLU()\n",
    "        self.dense2 = nn.Linear(ffn_num_hiddens, ffn_num_outputs)\n",
    "\n",
    "    def forward(self, X):\n",
    "        return self.dense2(self.gelu(self.dense1(X)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42c9d3c9-327a-4b86-b9f1-662de34ccc99",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DotProductAttention(nn.Module):\n",
    "    \"Scaled dot-product attention.\"\n",
    "    \n",
    "    def __init__(self, dropout, **kwargs):\n",
    "        super(DotProductAttention, self).__init__(**kwargs)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        \n",
    "    def forward(self, queries, keys, values):\n",
    "        d = queries.shape[-1]\n",
    "        scores = torch.bmm(queries, keys.transpose(1,2)) / math.sqrt(d)\n",
    "        self.attention_weights = nn.functional.softmax(scores, dim = -1)\n",
    "        return torch.bmm(self.dropout(self.attention_weights), values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96064717-da5e-411d-969a-b30537bbcd3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def transpose_qkv(X, num_heads):\n",
    "    \"Reshape for parallel computation across multiple attention heads.\"\n",
    "    X = X.reshape(X.shape[0], X.shape[1], num_heads, -1)\n",
    "    X = X.permute(0, 2, 1, 3)\n",
    "    return X.reshape(-1, X.shape[2], X.shape[3])\n",
    "\n",
    "def transpose_output(X, num_heads):\n",
    "    \"Reverse the operations of transpose_qkv.\"\n",
    "    X = X.reshape(-1, num_heads, X.shape[1], X.shape[2])\n",
    "    X = X.permute(0, 2, 1, 3)\n",
    "    return X.reshape(X.shape[0], X.shape[1], -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "725b16e6-7d26-4f2d-a43a-21dd5852d434",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadAttention_1(nn.Module):\n",
    "    \"Multi-head attention.\"\n",
    "    def __init__(self, key_size, query_size, value_size, num_hiddens,\n",
    "                 num_heads, dropout, bias=False, **kwargs):\n",
    "        super(MultiHeadAttention_1, self).__init__(**kwargs)\n",
    "        self.num_heads = num_heads\n",
    "        self.attention = DotProductAttention(dropout)\n",
    "        self.W_q = nn.Linear(num_hiddens, num_hiddens, bias=bias)\n",
    "        self.W_k = nn.Linear(num_hiddens, num_hiddens, bias=bias)\n",
    "        self.W_v = nn.Linear(num_hiddens, num_hiddens, bias=bias)\n",
    "        self.W_o = nn.Linear(num_hiddens, num_hiddens, bias=bias)\n",
    "\n",
    "    def forward(self, queries, keys, values):\n",
    "        \n",
    "        queries = transpose_qkv(self.W_q(queries), self.num_heads)\n",
    "        keys = transpose_qkv(self.W_k(keys), self.num_heads)\n",
    "        values = transpose_qkv(self.W_v(values), self.num_heads)\n",
    "        output = self.attention(queries, keys, values)\n",
    "        output_concat = transpose_output(output, self.num_heads)\n",
    "        return self.W_o(output_concat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff66d371-935e-4eb6-a8ca-2c031cadb35f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class EncoderBlock_1(nn.Module):\n",
    "    \"Transformer encoder block.\"\n",
    "    def __init__(self, key_size, query_size, value_size, num_hiddens, norm_shape,\n",
    "                 ffn_num_input, ffn_num_hiddens, ffn_num_outputs, num_heads, dropout, use_bias=False, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.attention = MultiHeadAttention_1(key_size, query_size, value_size, num_hiddens, num_heads, dropout)\n",
    "        self.addnorm1 = AddNorm(norm_shape, dropout)\n",
    "        self.ffn = PositionWiseFFN(ffn_num_input, ffn_num_hiddens, num_hiddens)\n",
    "        self.addnorm2 = AddNorm(norm_shape, dropout)\n",
    "\n",
    "    def forward(self, X):\n",
    "        Y = self.addnorm1(X, self.attention(X, X, X))\n",
    "        return self.addnorm2(Y, self.ffn(Y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e96d68ae-d827-4d06-9998-fa8275f7931d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class iTransformer_1(nn.Module):\n",
    "    \"Transformer encoder.\"\n",
    "    def __init__(self, vocab_size, key_size, query_size, value_size,\n",
    "                 num_hiddensT, norm_shape, ffn_num_input, ffn_num_hiddens, ffn_num_outputs,\n",
    "                 num_headsT, num_layersT, pred_len, dropout = 0, use_bias=False, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.vocab_size = vocab_size\n",
    "        self.linear = nn.Linear(num_hiddensT, pred_len)\n",
    "        self.fc = nn.Linear(vocab_size, num_hiddensT)\n",
    "        self.blks = nn.Sequential()\n",
    "        for i in range(num_layersT):\n",
    "            self.blks.add_module(\"block\"+str(i),\n",
    "                EncoderBlock_1(key_size, query_size, value_size, num_hiddensT,\n",
    "                             norm_shape, ffn_num_input, ffn_num_hiddens, ffn_num_outputs,\n",
    "                             num_headsT, dropout, use_bias))\n",
    "    def forward(self, X, *args):\n",
    "        X = self.fc(X)\n",
    "        for i, blk in enumerate(self.blks):\n",
    "            X = blk(X)\n",
    "        return self.linear(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc3a58f9-749c-4f07-a291-8366f2ad2b4e",
   "metadata": {},
   "source": [
    "<span style = 'color:red;font-size:18px'>Spatiotemporal fusion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78fdfdfd-38b4-4da1-9212-6ebc98ee4b3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class STE_block(nn.Module):\n",
    "    def __init__(self, hidden_dimS, num_heads, embedding_dim2, num_layersl, hidden_sizel, kernel_size, \n",
    "                 hidden_dimT, num_layersT, ffn_num_hiddens, num_headsT, input_len, pred_len):\n",
    "        super(STE_block, self).__init__()\n",
    "       \n",
    "        # Heterogeneous graph network.\n",
    "        self.S_GAT1 = H_Model(hidden_dimS, num_heads, embedding_dim2)\n",
    "        self.hidden_sizel = hidden_sizel\n",
    "       \n",
    "        # Temporal–feature attention mechanism.\n",
    "        self.Lstm1 = LSTMNetwork(hidden_dimS, hidden_sizel, num_layersl)\n",
    "        self.iTsf = iTransformer_1(input_len, input_len, input_len, input_len, hidden_dimT, hidden_dimT, \n",
    "                                   hidden_dimT, ffn_num_hiddens, hidden_dimT, num_headsT, num_layersT, pred_len)\n",
    "        # Coordinate attention.\n",
    "        self.att1 = AttentionModule(kernel_size, pool_dim1 = 1, pool_dim2 = 3)\n",
    "        self.fc = nn.Linear(embedding_dim2 * 4, hidden_dimS)\n",
    "        \n",
    "    def forward(self, input_x, data_D, device, masks_x, distances): \n",
    " \n",
    "        batch_size, time_steps, num_nodes, _ = input_x.shape\n",
    "        x_d, x_s = self.fc(data_D['DYA'].x), self.fc(data_D['STA'].x)\n",
    "        x_dictD0 = {'DYA': x_d, 'STA': x_s}\n",
    "        \n",
    "        \"Spatial feature mining\"\n",
    "        X_in = torch.cat((data_D['DYA'].x, data_D['STA'].x), dim = -1).reshape(batch_size, time_steps, num_nodes, -1)\n",
    "        X_in = X_in.permute(0, 2, 3, 1) # reshape(batch_size, num_nodes, feature_dim, time_steps)\n",
    "        x_dynamic0, x_static0 = self.S_GAT1(x_dictD0, data_D)  # reshape(batch_size * time_steps * 节点数, feature_dim)\n",
    "        \n",
    "        \"Coordinate attention computation\"\n",
    "        X_dynamic = x_dictD0['DYA'].reshape(batch_size, time_steps, num_nodes, -1)\n",
    "        X_static = x_dictD0['STA'].reshape(batch_size, time_steps, num_nodes, -1)\n",
    "        X_SD = torch.cat((X_dynamic, X_static), dim = -1).permute(0, 3, 2, 1)\n",
    "        X_SD0 = self.att1(X_SD, masks_x, device)  # reshape(batch_size, 2*feature_dim, num_nodes, time_steps)\n",
    "        F = X_SD0.shape[1]\n",
    "        X_SD0 = X_SD0.permute(0, 3, 2, 1).reshape(batch_size * time_steps * num_nodes, -1) # reshape(B*T*N, F)   \n",
    "        \n",
    "        \"Feature fusion\"\n",
    "        X_D0, X_S0 = X_SD0[:, : F // 2], X_SD0[:, F // 2:]\n",
    "        X_D1, X_S1 = x_dynamic0 + X_D0, x_static0 + X_S0\n",
    "        X_D1 = X_D1.reshape(batch_size, time_steps, num_nodes, -1).permute(0, 2, 1, 3).reshape(batch_size * num_nodes, time_steps, -1)\n",
    "        X_S1 = X_S1.reshape(batch_size, time_steps, num_nodes, -1).permute(0, 2, 1, 3).reshape(batch_size * num_nodes, time_steps, -1)\n",
    "        _, h_d, c_d = self.Lstm1(X_D1)\n",
    "        _, h_s, c_s = self.Lstm1(X_S1)\n",
    "        \n",
    "        \"Long-range temporal modeling\"\n",
    "        X_SD1 = torch.cat((data_D['DYA'].x, data_D['STA'].x), dim = -1).reshape(batch_size, time_steps, num_nodes, -1).permute(0, 2, 3, 1)  \n",
    "        X_SD1 = X_SD1.reshape(batch_size * num_nodes, -1, time_steps)                                  \n",
    "       \n",
    "        output = self.iTsf(X_SD1) # shape(batch_size * num_nodes, featrure_dim, time_steps)                                  \n",
    "        \n",
    "        # shape(batch_size * num_nodes, featrure_dim, time_steps)   \n",
    "        return h_d + h_s, c_d + c_s, output"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e3dc384-3d9e-465a-ad0a-95ef007d43f5",
   "metadata": {},
   "source": [
    "<span style = 'color:red;font-size:20px'>HSTDFormer decoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f45db34a-c3cb-4dfb-89ad-ff0c955e5d4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DecoderModel(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, num_layers, output_dim):\n",
    "        \"\"\"\n",
    "        Decoder model\n",
    "        param input_dim: input feature dimension\n",
    "        param hidden_dim: hidden-layer dimension\n",
    "        param num_layers: number of LSTM layers\n",
    "        param output_dim: output feature dimension\n",
    "        \"\"\"\n",
    "        super(DecoderModel, self).__init__()\n",
    "        self.lstm = nn.LSTM(input_dim, hidden_dim, num_layers, batch_first=True)\n",
    "        self.fc = nn.Linear(hidden_dim, output_dim)\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.num_layers = num_layers\n",
    "\n",
    "    def forward(self, target, h, c):\n",
    "        \"\"\"\n",
    "        Forward pass for training.\n",
    "        param input_x: initial decoder input, shape (time_steps, batch_size, input_dim)\n",
    "        param hidden: initial decoder hidden state, shape (1, batch_size, hidden_dim)\n",
    "        param cell: initial decoder cell state, shape (1, batch_size, hidden_dim)\n",
    "        param target: target sequence for training, shape (target_len, batch_size, input_dim)\n",
    "        return: output sequence, shape (batch_size, target_len, output_dim)\n",
    "        \"\"\"\n",
    "        batch_size = target.size(0)\n",
    "        target_len = target.size(1)\n",
    "        outputs = torch.zeros(batch_size, target_len, self.fc.out_features).to(h.device)\n",
    "        hidden = h\n",
    "        cell = c\n",
    "        output, (hidden, cell) = self.lstm(target, (hidden.contiguous(), cell.contiguous())) \n",
    "        outputs = self.fc(output)  \n",
    "        \n",
    "        return outputs   #shape (batch_size, target_len, output_dim)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae5eed7c-0816-4b5f-960d-f486045e2365",
   "metadata": {},
   "source": [
    "<span style = 'color:red;font-size:18px'>Heterogeneous-graph static feature embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "565bfe4e-56cc-4b88-9a55-443c372d0d76",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Embedding(torch.nn.Module):\n",
    "    def __init__(self, embedding_dim2):\n",
    "        super(Embedding, self).__init__()\n",
    "\n",
    "        # Static-data encoder layer\n",
    "        # Denmark\n",
    "        self.embedding_layer1 = nn.Embedding(num_embeddings=21, embedding_dim=embedding_dim2)  # Vessel type\n",
    "        self.embedding_layer2 = nn.Embedding(num_embeddings=65, embedding_dim=embedding_dim2)  # Length\n",
    "        self.embedding_layer3 = nn.Embedding(num_embeddings=420, embedding_dim=embedding_dim2) # Width \n",
    "        self.embedding_layer4 = nn.Embedding(num_embeddings=15, embedding_dim=embedding_dim2)  # Draft\n",
    "        \n",
    "        # # California\n",
    "        # self.embedding_layer1 = nn.Embedding(num_embeddings=91, embedding_dim=embedding_dim2)  \n",
    "        # self.embedding_layer2 = nn.Embedding(num_embeddings=61, embedding_dim=embedding_dim2)  \n",
    "        # self.embedding_layer3 = nn.Embedding(num_embeddings=401, embedding_dim=embedding_dim2) \n",
    "        # self.embedding_layer4 = nn.Embedding(num_embeddings=23, embedding_dim=embedding_dim2) \n",
    "\n",
    "        # # Houston\n",
    "        # self.embedding_layer1 = nn.Embedding(num_embeddings=100, embedding_dim=embedding_dim2)  \n",
    "        # self.embedding_layer2 = nn.Embedding(num_embeddings=67, embedding_dim=embedding_dim2)   \n",
    "        # self.embedding_layer3 = nn.Embedding(num_embeddings=365, embedding_dim=embedding_dim2)  \n",
    "        # self.embedding_layer4 = nn.Embedding(num_embeddings=23, embedding_dim=embedding_dim2)   \n",
    "        \n",
    "    def forward(self, dataD, device):\n",
    "                \n",
    "        dataD = dataD.to(device)\n",
    "        self.xd_s = dataD['STA'].x\n",
    "\n",
    "        # Static attribute encoding\n",
    "        self.Xd_S1 = self.embedding_layer1(self.xd_s[:, 0].long())  # Vessel type\n",
    "        self.Xd_S2 = self.embedding_layer2(self.xd_s[:, 1].long())  # Length\n",
    "        self.Xd_S3 = self.embedding_layer3(self.xd_s[:, 2].long())  # Width \n",
    "        self.Xd_S4 = self.embedding_layer4(self.xd_s[:, 3].long())  # Draft\n",
    "\n",
    "        self.Xd_S = torch.cat((self.Xd_S1, self.Xd_S2, self.Xd_S3, self.Xd_S4), dim=1)\n",
    "        dataD['STA'].x = self.Xd_S\n",
    "        \n",
    "        return dataD"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49de10d0-09e1-4a28-89ff-b278ed6382be",
   "metadata": {},
   "source": [
    "<span style = 'color:red;font-size:18px'>Prediction padding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "adbc27d4-50aa-454b-a013-d98f920b5190",
   "metadata": {},
   "outputs": [],
   "source": [
    "def ipad_out(Y, masks, device):\n",
    "    batch_size, pred_time_steps, max_nodes = masks.shape\n",
    "    _, _, feature_dim = Y.shape\n",
    "    \n",
    "    # 1) Find valid nodes in each batch\n",
    "    valid_nodes_masks = masks.all(dim=1)\n",
    "    valid_nodes_indices = [torch.where(valid_nodes_masks[b])[0] for b in range(batch_size)] \n",
    "    \n",
    "    # 2) Initialize an all-zero tensor\n",
    "    output = torch.zeros(batch_size, max_nodes, pred_time_steps, feature_dim).to(device) \n",
    "    \n",
    "    # 3) Iterate over each batch and fill the data\n",
    "    start_idx = 0\n",
    "    for b in range(batch_size):\n",
    "        valid_indices = valid_nodes_indices[b] \n",
    "        num_valid_nodes = len(valid_indices)   \n",
    "        Y_batch = Y[start_idx:start_idx + num_valid_nodes]  \n",
    "        start_idx += num_valid_nodes\n",
    "        output[b, valid_indices, :, :] = Y_batch \n",
    "    return output"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61e5a605-b995-41a0-bb03-2dd8844fb4ef",
   "metadata": {},
   "source": [
    "<span style = 'color:red;font-size:25px'>Temporal encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "193989a0-13f2-4a80-8e9e-841acc3df315",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_positional_encoding(num_steps, hidden_dim, device):\n",
    "    \"\"\"\n",
    "    Generate sinusoidal positional encoding.\n",
    "    \n",
    "    Parameters：\n",
    "    num_steps: number of time steps\n",
    "    hidden_dim: feature dimension\n",
    "    device: compute device\n",
    "\n",
    "    Return:\n",
    "    positional encoding tensor of shape (num_steps, hidden_dim)\n",
    "    \"\"\"\n",
    "    position = torch.arange(num_steps, dtype=torch.float, device=device).unsqueeze(1)\n",
    "    div_term = torch.exp(torch.arange(0, hidden_dim, 2, dtype=torch.float, device=device) * \n",
    "                         -(torch.log(torch.tensor(10000.0, device=device)) / hidden_dim))\n",
    "    pe = torch.zeros(num_steps, hidden_dim, device=device)\n",
    "    pe[:, 0::2] = torch.sin(position * div_term)  \n",
    "    pe[:, 1::2] = torch.cos(position * div_term)  \n",
    "\n",
    "    return pe\n",
    "\n",
    "def add_positional_encoding(static_features, device):\n",
    "    \"\"\"\n",
    "    Combine positional encoding with static-node features.\n",
    "\n",
    "    Parameters:：\n",
    "    static_features: input static-feature tensor of shape (batch_size * num_nodes, num_steps, hidden_dim)\n",
    "\n",
    "    Return:\n",
    "    static features with positional encoding added; shape unchanged\n",
    "    \"\"\"\n",
    "    batch_node_size, num_steps, hidden_dim = static_features.shape \n",
    "    positional_encoding = generate_positional_encoding(num_steps, hidden_dim, device)   \n",
    "    static_features_with_encoding = static_features + positional_encoding.unsqueeze(0)\n",
    "\n",
    "    return static_features_with_encoding"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cfee957e-4ed7-48ae-bca0-36ec40473a7c",
   "metadata": {},
   "source": [
    "# <span style = 'color:red'>Main model function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6400d9d5-90c3-4c15-b6ba-c17fed6a1987",
   "metadata": {},
   "outputs": [],
   "source": [
    "class H_data(nn.Module):\n",
    "    def __init__(self,input_len,pred_len,input_dimD,hidden_dimD,embedding_dim1,embedding_dim2,hidden_dimS,num_heads,num_layersl,\n",
    "                 hidden_sizel,kernel_size,hidden_dimT,num_layersT,ffn_num_hiddens,num_headsT,num_layersl2, hidden_sizel2):\n",
    "        super(H_data, self).__init__()\n",
    "        \n",
    "        self.input_len = input_len\n",
    "        self.pred_len = pred_len\n",
    "        self.embed = Embedding(embedding_dim2)\n",
    "        self.STE1 = STE_block(hidden_dimS, num_heads, embedding_dim2, num_layersl2, hidden_sizel2, kernel_size, \n",
    "                 hidden_dimT, num_layersT, ffn_num_hiddens, num_headsT, input_len, pred_len)   \n",
    "        self.lstm = LSTMNetwork(input_dimD, hidden_sizel, num_layersl)\n",
    "        self.decoder = DecoderModel(embedding_dim2 * 8, hidden_sizel2, num_layersl2, output_dim = 2)\n",
    "        \n",
    "    def forward(self, input_x, static_X, device, F, S, batch_size, masks_x, distances): \n",
    "        \"\"\"\n",
    "        input_x is a list of batches with length batch_size, where each element contains total_len time steps; \n",
    "        overall shape: (batch_size, time_steps, num_nodes, feature_dim)\n",
    "        static_X_list shape: (batch_size, time_steps, num_nodes, feature_dim)\n",
    "        F shape: (batch_size, node_id)\n",
    "        S shape: (batch_size, time_steps, node_id)\n",
    "        masks_x shape: (batch_size, time_steps, num_nodes)\n",
    "        \"\"\"\n",
    "        batch_size, time_steps, num_nodes, _ = input_x.shape\n",
    "        data_D, batch = HeteroGraphBuilder_batch(input_x, static_X, masks_x, device, distances)\n",
    "        data_D = self.embed(data_D, device)\n",
    "        X_dynamic = data_D['DYA'].x\n",
    "        X_static = data_D['STA'].x\n",
    "        x_dynamic1, x_static1 = X_dynamic.reshape(batch_size, time_steps, num_nodes, -1), X_static.reshape(batch_size, time_steps, num_nodes, -1)\n",
    "        x_dynamic1, x_static1 = x_dynamic1.permute(0, 2, 1, 3), x_static1.permute(0, 2, 1, 3) \n",
    "        x_dynamic1, x_static1 = x_dynamic1.reshape(-1, time_steps, x_dynamic1.shape[-1]), x_static1.reshape(-1, time_steps, x_static1.shape[-1])\n",
    "        \n",
    "        # Use an LSTM to capture temporal features\n",
    "        x_sta1 = add_positional_encoding(x_static1, device)\n",
    "        x_dyn1, _, _ = self.lstm(x_dynamic1)\n",
    "        feature_dim = x_dyn1.shape[-1]\n",
    "        \n",
    "        # reshape(batch_size*time_steps*num_nodes, -1)\n",
    "        x_dyn1 = x_dyn1.reshape(batch_size, num_nodes, time_steps, -1).permute(0, 2, 1, 3).reshape(-1, feature_dim)\n",
    "        x_sta1 = x_sta1.reshape(batch_size, num_nodes, time_steps, -1).permute(0, 2, 1, 3).reshape(-1, feature_dim)\n",
    "        data_D['DYA'].x, data_D['STA'].x = x_dyn1, x_sta1\n",
    "\n",
    "        h_D, c_D, X_din = self.STE1(input_x, data_D, device, masks_x, distances) # shape(B * N, F, T)\n",
    "        X_din = X_din.permute(0, 2, 1)\n",
    "\n",
    "        # Decode the outputs\n",
    "        Y_h = self.decoder(X_din, h_D, c_D)  \n",
    "        Y_hat = Y_h.reshape(batch_size, num_nodes, self.pred_len, -1) \n",
    "        masks_out = masks_x[:, :self.pred_len, :].to(device)\n",
    "\n",
    "        return Y_hat, masks_out          # Y_hat shape:(batch_size, max_nodes, time_steps, feature_dim),\n",
    "                                         # masks_out shape: (batch_size, time_steps, max_nodes)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ce4bc82-999d-4bcb-94c9-50394cb2ef54",
   "metadata": {},
   "source": [
    "<span style = 'color:red; font-size:18px'>Display training results in real time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b1e514a-efe2-4261-b263-4f5a87082954",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Animator:  #@save\n",
    "    \n",
    "    \"\"\"在动画中绘制数据\"\"\"\n",
    "    def __init__(self, xlabel=None, ylabel=None, legend=None, xlim=None,\n",
    "                 ylim=None, xscale='linear', yscale='linear',\n",
    "                 fmts=('-', 'm--', 'g-.', 'r:'), nrows=1, ncols=1,\n",
    "                 figsize=(3.5, 2.5)):\n",
    "        if legend is None:\n",
    "            legend = []\n",
    "        d2l.use_svg_display()\n",
    "        self.fig, self.axes = d2l.plt.subplots(nrows, ncols, figsize=figsize)\n",
    "        if nrows * ncols == 1:\n",
    "            self.axes = [self.axes, ]\n",
    "        self.config_axes = lambda: d2l.set_axes(\n",
    "            self.axes[0], xlabel, ylabel, xlim, ylim, xscale, yscale, legend)\n",
    "        self.X, self.Y, self.fmts = None, None, fmts\n",
    "\n",
    "    def add(self, x, y):\n",
    "        if not hasattr(y, \"__len__\"):\n",
    "            y = [y]\n",
    "        n = len(y)\n",
    "        if not hasattr(x, \"__len__\"):\n",
    "            x = [x] * n\n",
    "        if not self.X:\n",
    "            self.X = [[] for _ in range(n)]\n",
    "        if not self.Y:\n",
    "            self.Y = [[] for _ in range(n)]\n",
    "        for i, (a, b) in enumerate(zip(x, y)):\n",
    "            if a is not None and b is not None:\n",
    "                self.X[i].append(a)\n",
    "                self.Y[i].append(b)\n",
    "        self.axes[0].cla()\n",
    "        for x, y, fmt in zip(self.X, self.Y, self.fmts):\n",
    "            self.axes[0].plot(x, y, fmt)\n",
    "        self.config_axes()\n",
    "        display.display(self.fig)\n",
    "        display.clear_output(wait=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84a4be3e-6f06-47e5-ab22-8951e80354fa",
   "metadata": {},
   "source": [
    "# <span style = 'color:red;font-size:18px'>Pad the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d759817-a76b-43f2-94ed-f34107ee8cc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def pad_outputy(output_Y):\n",
    "    \"\"\"\n",
    "    Pad an irregular list of tensors of shape (batch_size, time_steps, num_nodes, features) and generate a mask.\n",
    "    \n",
    "    :param output_Y: a list where each element has shape (time_steps, num_nodes, features))\n",
    "    :return: \n",
    "        - padded_output: padded tensor of shape (batch_size, time_steps, max_nodes, features)\n",
    "        - masks: padding mask of shape (batch_size, time_steps, max_nodes)\n",
    "    \"\"\"\n",
    "    if not output_Y or any(len(y) == 0 for y in output_Y):\n",
    "        return torch.empty(0), torch.empty(0)\n",
    "    \n",
    "    output_Y = [np.array(y) for y in output_Y if len(y) > 0]\n",
    "    max_nodes = max(y.shape[1] for y in output_Y)\n",
    "    feature_dim = output_Y[0].shape[2]\n",
    "    max_time_steps = max(y.shape[0] for y in output_Y)\n",
    "    batch_size = len(output_Y)\n",
    "\n",
    "    padded_output = torch.zeros((batch_size, max_time_steps, max_nodes, feature_dim), dtype=torch.float32)\n",
    "    masks = torch.zeros((batch_size, max_time_steps, max_nodes), dtype=torch.float32)\n",
    "    \n",
    "    for i, batch in enumerate(output_Y):\n",
    "        t, n, f = batch.shape\n",
    "        padded_output[i, :t, :n, :] = torch.tensor(batch, dtype=torch.float32)\n",
    "        masks[i, :t, :n] = 1.0\n",
    "\n",
    "    return padded_output, masks\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8dfc88c5-3617-4f3e-a336-f671f701dd0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def pad_output(output_Y):\n",
    "    \"\"\"\n",
    "    Pad irregular input data of shape (batch_size, [list of time steps], num_nodes, features) and generate a mask.\n",
    "    \n",
    "    :param output_Y: a nested list of NumPy arrays or tensors, where each element is an array of shape (num_nodes, features)\n",
    "    :return: \n",
    "        - padded_output: padded tensor of shape (batch_size, max_time_steps, max_nodes, feature_dim)\n",
    "        - masks: padding mask of shape (batch_size, max_time_steps, max_nodes)\n",
    "    \"\"\"\n",
    "    batch_size = len(output_Y)\n",
    "    time_steps_list = [len(batch) for batch in output_Y]\n",
    "    max_time_steps = max(time_steps_list)  # Maximum number of time steps\n",
    "\n",
    "    # Find the maximum number of nodes and the feature dimension\n",
    "    max_nodes = max([array.shape[0] for batch in output_Y for array in batch])\n",
    "    feature_dim = max([array.shape[1] for batch in output_Y for array in batch])\n",
    "\n",
    "    # Initialize the padded tensor and mask\n",
    "    padded_output = torch.zeros((batch_size, max_time_steps, max_nodes, feature_dim), dtype=torch.float32)\n",
    "    masks = torch.zeros((batch_size, max_time_steps, max_nodes), dtype=torch.float32)\n",
    "    \n",
    "    # Pad the data and generate the mask\n",
    "    for i, batch in enumerate(output_Y):\n",
    "        for t, array in enumerate(batch):\n",
    "            num_nodes, num_features = array.shape\n",
    "            padded_output[i, t, :num_nodes, :num_features] = torch.tensor(array, dtype=torch.float32)\n",
    "            masks[i, t, :num_nodes] = 1.0  \n",
    "    \n",
    "    return padded_output, masks"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c324e04b-8c96-42f9-a27c-249cf0bf1fa1",
   "metadata": {},
   "source": [
    "<span style = 'color:red;font-size:18px'>Model training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f224fbaa-3c81-444c-9916-d530e06ed0ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(net, lr, epochs, input_len, pred_len, file_path, file_vpath, file_pathS,\n",
    "                 weight_decay, max_values, min_values, batch_size, hidden_dimT):\n",
    "    \"Train the seq2seq model\"\n",
    "    \n",
    "    def xavier_init_weights(m):\n",
    "        if isinstance(m, nn.Linear):\n",
    "            nn.init.xavier_uniform_(m.weight)\n",
    "\n",
    "    net.apply(xavier_init_weights)\n",
    "    \n",
    "    if torch.cuda.is_available():\n",
    "        device = torch.device('cuda')\n",
    "    torch.cuda.init()\n",
    "    \n",
    "    net.to(device)\n",
    "    \n",
    "    optimizer = torch.optim.Adam(net.parameters(), lr=lr, weight_decay=weight_decay)\n",
    "    scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=10, gamma=0.5)  \n",
    "    loss = nn.L1Loss()\n",
    "    \n",
    "    # Load and convert static data\n",
    "    dataS = extract_mmsi_features(file_pathS)\n",
    "    dataS = transform_data(dataS)\n",
    "    \n",
    "    net.train()\n",
    "    animator = Animator(xlabel='epoch', ylabel='loss', yscale='log', xlim=[0, epochs],\n",
    "                        legend=['train', 'valid'])\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        timer = d2l.Timer()\n",
    "        metric = d2l.Accumulator(2)         # Log L1 loss during training\n",
    "        metric_mae = d2l.Accumulator(2)     # MAE during training\n",
    "        metric_ade = d2l.Accumulator(2)     # ADE during training\n",
    "        metric_mvalid = d2l.Accumulator(2)  # Log validation MAE \n",
    "        metric_made = d2l.Accumulator(2)    # Log validation ADE\n",
    "        timer.start()\n",
    "        \n",
    "        # ------------------ Train------------------\n",
    "        for slice_y in slice_data_generator(file_path, input_len, pred_len, batch_size):\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            if len(slice_y) != batch_size:\n",
    "                break\n",
    "            \n",
    "            input_X, F, output_Y, S, _, input_x_all, Static_result_all = process_data(\n",
    "                slice_y, input_len, pred_len, dataS\n",
    "            )\n",
    "            if not input_x_all or all(len(batch) == 0 for batch in input_x_all):\n",
    "                break\n",
    "\n",
    "            In_x, masks_x = pad_output(input_x_all)\n",
    "            In_xr = In_x.reshape(-1, In_x.shape[3]).to(device)\n",
    "            masks_xr = masks_x.reshape(-1).bool()\n",
    "            np.seterr(divide='ignore')\n",
    "            distances = (1 / haversine_distances(In_xr)).to(device)\n",
    "            Out_Y, masks_Y0 = pad_outputy(output_Y)\n",
    "            if Out_Y.shape[0] == 0:\n",
    "                break\n",
    "            In_x1, Out_Y1 = In_x[:, :, :, :2].to(device), Out_Y[:, :, :, :2].to(device)\n",
    "            n_x = torch.cat((In_x1[:, 0].unsqueeze(1), In_x1[:, :-1]), dim=1)\n",
    "            n_y = torch.cat((Out_Y1[:, 0].unsqueeze(1), Out_Y1[:, :-1]), dim=1)\n",
    "            masks_y = masks_Y0.to(device)\n",
    "            in_x = normalize_datat(In_x, max_values, min_values)\n",
    "            Out_Y = normalize_datat(Out_Y, max_values, min_values)\n",
    "            sta_x, _ = pad_output(Static_result_all)\n",
    "            Y_h, _ = net(in_x, sta_x, device, F, S, batch_size, masks_x, distances)  \n",
    "            masks_Y = masks_y.bool().unsqueeze(-1).expand_as(Out_Y1)\n",
    "            masked_Y = Out_Y * masks_Y\n",
    "            masked_Y1 = Out_Y1 * masks_Y\n",
    "            masked_Y_hat = Y_hat * masks_Y\n",
    "            l = loss(masked_Y, masked_Y_hat)\n",
    "            \n",
    "            with torch.no_grad():\n",
    "                masked_Y_ht = denormalize_data(masked_Y_hat, max_values, min_values)\n",
    "                masked_Y_ht = masked_Y_ht * masks_Y\n",
    "                ADE_ = calculate_ade(masked_Y_ht, masked_Y1, masks_Y)\n",
    "                mae = torch.abs(masked_Y_ht - masked_Y1) * masks_Y\n",
    "                valid_count = masks_Y.sum().item()\n",
    "                metric.add(l.sum(), valid_count)\n",
    "                metric_mae.add(mae.sum(), valid_count) \n",
    "                metric_ade.add(ADE_.sum(), valid_count // 2) \n",
    "            l.sum().backward()\n",
    "            optimizer.step()\n",
    "        scheduler.step()  \n",
    "        \n",
    "        # ------------------ Valid ------------------\n",
    "        with torch.no_grad():\n",
    "            for slice_vy in slice_data_generator(file_vpath, input_len, pred_len, batch_size):\n",
    "                if len(slice_vy) != batch_size:\n",
    "                    break\n",
    "\n",
    "                input_vX, v_F, output_vY, v_S, _, input_vx_all, Static_vresult_all = process_data(\n",
    "                    slice_vy, input_len, pred_len, dataS\n",
    "                )\n",
    "                if not input_vx_all or all(len(batch) == 0 for batch in input_vx_all):\n",
    "                    break\n",
    "    \n",
    "                In_vx, masks_vx = pad_output(input_vx_all)\n",
    "                In_vxr = In_vx.reshape(-1, In_vx.shape[3]).to(device)\n",
    "                masks_vxr = masks_vx.reshape(-1).bool()\n",
    "                np.seterr(divide='ignore')\n",
    "                distances_v = (1 / haversine_distances(In_vxr)).to(device)\n",
    "                \n",
    "                Out_vY, masks_vY0 = pad_outputy(output_vY) \n",
    "                if Out_vY.shape[0] == 0:\n",
    "                    break\n",
    "                \n",
    "                In_vx1, Out_vY1 = In_vx[:, :, :, :2].to(device), Out_vY[:, :, :, :2].to(device)\n",
    "                n_vx = torch.cat((In_vx1[:, 0].unsqueeze(1), In_vx1[:, :-1]), dim=1)\n",
    "                n_vy = torch.cat((Out_vY1[:, 0].unsqueeze(1), Out_vY1[:, :-1]), dim=1)\n",
    "                \n",
    "                masks_vy = masks_vY0.to(device)\n",
    "                in_vx = normalize_datat(In_vx, max_values, min_values)\n",
    "                sta_vx, _ = pad_output(Static_vresult_all)\n",
    "    \n",
    "                vY_h, _ = net(in_vx, sta_vx, device, v_F, v_S, batch_size, masks_vx, distances_v)  \n",
    "                vY_hat = denormalize_data(vY_h, max_values, min_values)\n",
    "    \n",
    "                masks_vY = masks_vy.bool().unsqueeze(-1).expand_as(Out_vY1)\n",
    "                masked_vY = Out_vY1 * masks_vY\n",
    "                masked_vY_hat = vY_hat * masks_vY\n",
    "                vADE_ = calculate_ade(masked_vY_hat, masked_vY, masks_vY)\n",
    "                mae_v = torch.abs(masked_vY_hat - masked_vY) * masks_vY\n",
    "                valid_vcount = masks_vY.sum().item()\n",
    "                metric_mvalid.add(mae_v.sum(), valid_vcount)\n",
    "                metric_made.add(vADE_.sum(), valid_vcount // 2)\n",
    "        if (epoch + 1) % 1 == 0:\n",
    "                animator.add(epoch + 1, ((metric_mae[0]/metric_mae[1], metric_mvalid[0]/metric_mvalid[1])))\n",
    "        print(f'epoch{epoch+1}train_loss:{metric_mae[0]/metric_mae[1]}')\n",
    "        print(f'epoch{epoch+1}valid_loss:{metric_mvalid[0]/metric_mvalid[1]}')\n",
    "        print(f'epoch{epoch+1}train_loss:{metric_ade[0]/metric_ade[1]}')\n",
    "        print(f'epoch{epoch+1}valid_loss:{metric_made[0]/metric_made[1]}')\n",
    "        print(f\"Epoch {epoch + 1}, Time: {timer.stop():.1f} seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0574a2dc-536c-4d44-9424-3895e57355d4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "383c48e7-064c-426f-8e24-a37b5a448c2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_len = 48  # Input sequence length\n",
    "pred_len = 24   # Prediction horizon length\n",
    "num_heads = 2  # Number of graph-attention heads\n",
    "embedding_dim2 = 4  # Embedding dimension for other static features\n",
    "hidden_dimS = 32   # Hidden dimension for heterogeneous-graph features\n",
    "hidden_dimT = 128 # Hidden feature dimension\n",
    "num_layersS = 2   # Heterogeneous-graph layers\n",
    "num_layersT = 5   # iTransformer layers\n",
    "input_dimD = 5  # Feature dimension of dynamic features \n",
    "hidden_dimD = 12\n",
    "input_dimS = output_dimS = hidden_sizel = embedding_dim2 * 4  # Feature dimension of static input\n",
    "num_layersl = 2   # LSTM layers\n",
    "num_layersl2 = 4\n",
    "hidden_sizel2 = 128\n",
    "kernel_size = 3  # Kernel size\n",
    "ffn_num_hiddens = 256\n",
    "num_headsT = 4 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c42378cf-58ed-4b7f-8ee5-29a99bdf84b7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95ea5d04-d817-4b9f-80f8-bf188ecdd802",
   "metadata": {},
   "outputs": [],
   "source": [
    "net = H_data(input_len,pred_len,input_dimD,hidden_dimD,embedding_dim1,embedding_dim2,hidden_dimS,num_heads,num_layersl,\n",
    "                 hidden_sizel,kernel_size,hidden_dimT,num_layersT,ffn_num_hiddens,num_headsT,num_layersl2, hidden_sizel2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8843d7f9-0fb5-4d96-a090-283f4021f345",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49ddb78d-ebfd-40d1-aaec-a987fcb563f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path =  # Training dataset path\n",
    "file_vpath = # Validation dataset path\n",
    "file_tpath = # Test dataset path\n",
    "file_pathS = # Static data path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9ca1c38-3926-4822-969d-37075e307b90",
   "metadata": {},
   "outputs": [],
   "source": [
    "max_values = [57.1976249957228, 8.411638137361614, 29.770000000000003, 359.9, 180.0]    # Max-value list\n",
    "min_values = [54.959007898554255, 6.297554604493042, 0.5, 0.0, 0.0]                     # Min-value list\n",
    "weight_decay = 0   # Regularization\n",
    "lr = 0.0001        # Learning rate\n",
    "epochs = 100       # Number of epochs\n",
    "batch_size = 2     # Number of windows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc164902-493b-43d1-97f8-14797d610738",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_model(net, lr, epochs, input_len, pred_len, file_path, file_vpath, file_pathS, weight_decay, max_values, min_values, batch_size, hidden_dimT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05b81eea-b30d-41ee-8bc2-be9f9f1a95eb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "1259e0da-fb0b-403b-b8bf-d21468985261",
   "metadata": {},
   "source": [
    "<span style = 'color:red;font-size:18px'>Model Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "700db4b3-7e22-41a3-9844-e60837496f28",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_model(net, input_len, pred_len, file_tpath, file_pathS, \n",
    "                max_values, min_values, batch_size):\n",
    "    \"Test the model\"\n",
    "\n",
    "    # Device selection and initialization.\n",
    "    if torch.cuda.is_available():\n",
    "        device = torch.device('cuda')\n",
    "        torch.cuda.init()\n",
    "    else:\n",
    "        device = torch.device('cpu')\n",
    "    \n",
    "    net.to(device)\n",
    "\n",
    "    # Load and convert static data\n",
    "    dataS = extract_mmsi_features(file_pathS)\n",
    "    dataS = transform_data(dataS)\n",
    "    \n",
    "    net.eval()\n",
    "\n",
    "    timer = d2l.Timer()\n",
    "    metric_tmse = d2l.Accumulator(2)\n",
    "    metric_trmse = d2l.Accumulator(2)\n",
    "    metric_tmae = d2l.Accumulator(2)\n",
    "    metric_tade = d2l.Accumulator(2)\n",
    "\n",
    "    total_pred_time = 0.0     # Total inference time (s)\n",
    "    total_trajectories = 0.0  # Total number of trajectories (per your definition: valid_tcount / 24 / 2)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for slice_ty in slice_data_generator(file_tpath, input_len, pred_len, batch_size):\n",
    "            if len(slice_ty) != batch_size:\n",
    "                break\n",
    "\n",
    "            input_tX, t_F, output_tY, t_S, _, input_tx_all, Static_tresult_all = process_data(\n",
    "                slice_ty, input_len, pred_len, dataS\n",
    "            )\n",
    "            if not input_tx_all or all(len(batch) == 0 for batch in input_tx_all):\n",
    "                break\n",
    "\n",
    "            In_tx, masks_tx = pad_output(input_tx_all)\n",
    "            In_txr = In_tx.reshape(-1, In_tx.shape[3])\n",
    "            masks_txr = masks_tx.reshape(-1).bool()\n",
    "            np.seterr(divide='ignore')\n",
    "            distances_t = (1 / haversine_distances(In_txr)).to(device)\n",
    "            \n",
    "            Out_tY, masks_tY0 = pad_outputy(output_tY) \n",
    "            if Out_tY.shape[0] == 0:\n",
    "                break\n",
    "            \n",
    "            In_tx1, Out_tY1 = In_tx[:, :, :, :2].to(device), Out_tY[:, :, :, :2].to(device)\n",
    "            n_tx = torch.cat((In_tx1[:, 0].unsqueeze(1), In_tx1[:, :-1]), dim=1)\n",
    "            n_ty = torch.cat((Out_tY1[:, 0].unsqueeze(1), Out_tY1[:, :-1]), dim=1)\n",
    "            \n",
    "            masks_ty = masks_tY0.to(device)\n",
    "            in_tx = normalize_datat(In_tx, max_values, min_values)\n",
    "            sta_tx, _ = pad_output(Static_tresult_all)\n",
    "\n",
    "            # === Log inference time (GPU synchronization for accurate timing) ===\n",
    "            if device.type == \"cuda\":\n",
    "                torch.cuda.synchronize()\n",
    "            start_time = time.time()\n",
    "            tY_h, _, att_nt, att_fn = net(in_tx, sta_tx, device, t_F, t_S, batch_size, masks_tx, distances_t)\n",
    "\n",
    "            if device.type == \"cuda\":\n",
    "                torch.cuda.synchronize()\n",
    "            end_time = time.time()\n",
    "            # =========================================\n",
    "\n",
    "            tY_hat = denormalize_data(tY_h, max_values, min_values)\n",
    "            masks_tY = masks_ty.bool().unsqueeze(-1).expand_as(Out_tY1)\n",
    "            masked_tY = Out_tY1 * masks_tY\n",
    "            masked_tY_hat = tY_hat * masks_tY\n",
    "\n",
    "            tADE_ = calculate_ade(masked_tY_hat, masked_tY, masks_tY)\n",
    "            mae_t = torch.abs(masked_tY_hat - masked_tY) * masks_tY\n",
    "            mse_t = calculate_mse(masked_tY, masked_tY_hat)\n",
    "            rmse_t = torch.sqrt(mse_t)\n",
    "            valid_tcount = masks_tY.sum().item()\n",
    "\n",
    "            # ====== Inference time and trajectory statistics =====\n",
    "            total_pred_time += (end_time - start_time)\n",
    "            total_trajectories += valid_tcount / 24.0 / 2.0\n",
    "            # =================================\n",
    "\n",
    "            metric_tmse.add(mse_t.sum(), valid_tcount)\n",
    "            metric_trmse.add(rmse_t.sum(), valid_tcount)\n",
    "            metric_tmae.add(mae_t.sum(), valid_tcount)\n",
    "            metric_tade.add(tADE_.sum(), valid_tcount / 2.0)\n",
    "\n",
    "    print(f'test_mae:  {metric_tmae[0]/metric_tmae[1]:.6f}')\n",
    "    print(f'test_ade:  {metric_tade[0]/metric_tade[1]:.6f}m')\n",
    "    print(f'test_mse:  {metric_tmse[0]/metric_tmse[1]:.6f}')\n",
    "    print(f'test_rmse: {torch.sqrt(torch.tensor(metric_tmse[0]/metric_tmse[1])):.6f}')\n",
    "\n",
    "    print(f\"total_pred_time(s): {total_pred_time:.6f}\")\n",
    "    print(f\"total_trajectories: {total_trajectories:.6f}\")\n",
    "\n",
    "    # Average inference time per trajectory\n",
    "    if total_trajectories > 0:\n",
    "        avg_time_per_traj = total_pred_time / total_trajectories\n",
    "        print(f'Average inference time per trajectory: {avg_time_per_traj:.6f} seconds')\n",
    "\n",
    "        # Throughput (trajectories/s)\n",
    "        traj_per_sec = total_trajectories / total_pred_time if total_pred_time > 0 else float(\"inf\")\n",
    "        print(f'Throughput (trajectories/sec): {traj_per_sec:.6f}')\n",
    "    else:\n",
    "        print(\"No valid trajectory batches processed.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15d88344-47e7-4268-b617-70d5cbf3af85",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_model(net, input_len, pred_len, file_tpath, file_pathS, max_values, min_values, batch_size)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
