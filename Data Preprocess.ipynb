{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61734d54-687f-4a38-bf08-5fb16da87368",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import math\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from scipy.signal import medfilt\n",
    "from scipy.interpolate import InterpolatedUnivariateSpline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2230ba0-bada-49f7-9eb0-9718eec856c2",
   "metadata": {},
   "source": [
    "<span style = 'color:red;font-size:25px'>Extract data for the same MMSI."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b619a4c0-828d-4db3-be82-7898dedeeee5",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_files = [\n",
    "                             # File path\n",
    "]\n",
    "\n",
    "output_folder = r\"  \"          # Output directory path\n",
    "\n",
    "os.makedirs(output_folder, exist_ok=True)\n",
    "mmsi_data = {}\n",
    "\n",
    "# Iterate over each input file and group data by MMSI\n",
    "for file in input_files:\n",
    "    try:\n",
    "        df = pd.read_csv(file)\n",
    "        if 'MMSI' not in df.columns:\n",
    "            print(f\"File {file} does not contain an MMSI column; skipping...\")\n",
    "            continue  \n",
    "            \n",
    "        for mmsi, group in df.groupby('MMSI'):\n",
    "            if mmsi not in mmsi_data:\n",
    "                mmsi_data[mmsi] = group\n",
    "            else:\n",
    "                mmsi_data[mmsi] = pd.concat([mmsi_data[mmsi], group], ignore_index=True)\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing file {file}: {e}\")\n",
    "\n",
    "# Save each MMSI’s data to a separate CSV file\n",
    "for mmsi, data in mmsi_data.items():\n",
    "    output_file = os.path.join(output_folder, f\"{mmsi}.csv\")\n",
    "    data.to_csv(output_file, index=False)\n",
    "\n",
    "print(\"Data extraction and saving complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ac16931-d34a-4f2b-beac-09731e9b64a0",
   "metadata": {},
   "source": [
    "<span style = 'color:red;font-size:25px'>Remove outlier points"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b5d68e6-aed3-4449-85d1-b3db3b0e0464",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "folder_path = r\"  \"   # Folder path\n",
    "csv_files = [file for file in os.listdir(folder_path) if file.endswith(\".csv\")]\n",
    "\n",
    "# Iterate over each CSV file\n",
    "for csv_file in csv_files:\n",
    "    file_path = os.path.join(folder_path, csv_file)\n",
    "    data = pd.read_csv(file_path)\n",
    "    if 'BaseDateTime' in data.columns:\n",
    "        data['BaseDateTime'] = pd.to_datetime(data['BaseDateTime'], format='%Y-%m-%dT%H:%M:%S', errors='coerce')\n",
    "        data.sort_values('BaseDateTime', inplace=True)\n",
    "    \n",
    "    data = data[(data['SOG'] >= 0.5) & (data['SOG'] <= 30)]\n",
    "    data = data[(data['COG'] >= 0) & (data['COG'] <= 360)]\n",
    "    data = data[(data['LAT'] >= -90) & (data['LAT'] <= 90)]\n",
    "    data = data[(data['LON'] >= 0) & (data['LON'] <= 180)]\n",
    "    data = data[(data['Heading'] >= 0) & (data['Heading'] <= 180)]\n",
    "    \n",
    "    # Overwrite the original CSV file\n",
    "    data.to_csv(file_path, index=False)\n",
    "\n",
    "print(\"Data processing complete.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a83975d-6c13-4c86-bb05-b43c8985d2d2",
   "metadata": {},
   "source": [
    "<span style = 'color:red;font-size:25px'>Fill static data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38698234-26de-43c4-a399-5c1b425a7a47",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Define target columns\n",
    "target_columns = ['VesselType', 'Width', 'Length', 'Draft']\n",
    "folder_path = r\"   \"\n",
    "csv_files = [file for file in os.listdir(folder_path) if file.endswith('.csv')]\n",
    "\n",
    "# Iterate over each CSV file in the folder\n",
    "for csv_file in csv_files:\n",
    "    file_path = os.path.join(folder_path, csv_file)\n",
    "    df = pd.read_csv(file_path)\n",
    "    valid_values = {}\n",
    "    \n",
    "    for column in target_columns:\n",
    "        if column in df.columns:\n",
    "            valid_value = df[column][(df[column].notna()) & ~(df[column].isin(['undefined', 'Undefined', '']))]\n",
    "            if not valid_value.empty:\n",
    "                valid_values[column] = valid_value.iloc[0]\n",
    "            else:\n",
    "                valid_values[column] = None  \n",
    "\n",
    "    # Fill target columns with the extracted valid values\n",
    "    for column in target_columns:\n",
    "        if column in valid_values and valid_values[column] is not None:\n",
    "            df[column] = valid_values[column]\n",
    "    \n",
    "    df.to_csv(file_path, index=False)\n",
    "    print(f\"Target columns have been filled and saved to {file_path}\")\n",
    "\n",
    "print(\"All CSV files have been processed.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0d62b23-415f-440f-97ce-2edeeb45236f",
   "metadata": {},
   "source": [
    "<span style = 'color:red;font-size:25px'>Convert timestamp types"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad35ff68-7a59-4f5c-93ed-2784509e6e9c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "folder_path = \"  \"  # Folder path\n",
    "csv_files = [file for file in os.listdir(folder_path) if file.endswith(\".csv\")]\n",
    "\n",
    "# Iterate over all CSV files\n",
    "for csv_file in csv_files:\n",
    "    file_path = os.path.join(folder_path, csv_file)\n",
    "    data = pd.read_csv(file_path)\n",
    "    \n",
    "    if 'BaseDateTime' in data.columns:\n",
    "        try:\n",
    "            data['BaseDateTime'] = pd.to_datetime(data['BaseDateTime'], format='%Y/%m/%d %H:%M:%S', errors='raise')\n",
    "        except ValueError:\n",
    "            data['BaseDateTime'] = pd.to_datetime(data['BaseDateTime'], errors='coerce')\n",
    "            data['BaseDateTime'] = data['BaseDateTime'].dt.strftime('%Y/%m/%d %H:%M:%S')\n",
    "        data.to_csv(file_path, index=False)\n",
    "\n",
    "print(\"Timestamps in the CSV files have been updated as needed.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e09d0696-a45d-46b5-8af5-cec872570742",
   "metadata": {},
   "source": [
    "<span style = 'color:red;font-size:25px'>Split the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b031ad4c-309b-4e9f-9534-2c49a8f36e2c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "folder_path = \"  \"    # Folder path\n",
    "csv_files = [file for file in os.listdir(folder_path) if file.endswith(\".csv\")]\n",
    "\n",
    "# Iterate over all CSV files\n",
    "for csv_file in csv_files:\n",
    "    file_path = os.path.join(folder_path, csv_file)\n",
    "    data = pd.read_csv(file_path)\n",
    "    segment_start = 0\n",
    "    segments = []\n",
    "\n",
    "    # Traverse the data and compute time differences between adjacent records\n",
    "    for i in range(1, len(data)):\n",
    "        timestamp1 = pd.to_datetime(data['BaseDateTime'].iloc[i - 1], format='%Y/%m/%d %H:%M:%S')\n",
    "        timestamp2 = pd.to_datetime(data['BaseDateTime'].iloc[i], format='%Y/%m/%d %H:%M:%S')\n",
    "        \n",
    "        if pd.isna(timestamp1) or pd.isna(timestamp2):\n",
    "            continue \n",
    "            \n",
    "        timestamp_diff = (timestamp2 - timestamp1).total_seconds() / 3600 \n",
    "\n",
    "        # If the time gap between two adjacent records exceeds 6 hours, split the data\n",
    "        if timestamp_diff > 6:\n",
    "            segment_data = data.iloc[segment_start:i]\n",
    "            segments.append(segment_data)\n",
    "            segment_start = i\n",
    "\n",
    "    if len(segments) > 0:\n",
    "        \n",
    "        # Save each segment to a new CSV file\n",
    "        for idx, segment_data in enumerate(segments):\n",
    "            mmsi_number = data['MMSI'].iloc[0] if 'MMSI' in data.columns else 'Unknown'\n",
    "            segment_filename = f\"{mmsi_number}_{idx + 1}.csv\"\n",
    "            segment_path = os.path.join(folder_path, segment_filename)\n",
    "            segment_data.to_csv(segment_path, index=False)\n",
    "\n",
    "        # Remove from the original file the rows with '# Timestamp' values that were saved\n",
    "        for segment in segments:\n",
    "            data = data[~data['BaseDateTime'].isin(segment['BaseDateTime'])]\n",
    "\n",
    "        data.to_csv(file_path, index=False)\n",
    "\n",
    "print(\"CSV data have been segmented by # Timestamp; the segments were saved to new files and removed from the original files,\" \n",
    "       \"and rows with matching '# Timestamp' values were deleted.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a496f86-ebf6-422b-ba2c-091f85a0bb42",
   "metadata": {},
   "source": [
    "<span style = 'color:red;font-size:25px'>Remove outlier points and duplicate-timestamp points"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f79d282f-1ff4-4767-babb-4f2cd3812688",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_distance(lon1, lat1, lon2, lat2):\n",
    "    \"\"\"\n",
    "    Compute the distance between two lon/lat points (km)\n",
    "    param lon1: longitude of point A\n",
    "    param lat1: latitude of point A\n",
    "    param lon2: longitude of point B\n",
    "    param lat2: latitude of point B\n",
    "    return: distance (km)\n",
    "    \"\"\"\n",
    "    EARTH_RADIUS = 6371  # Earth radius\n",
    "    lon1, lat1, lon2, lat2 = map(math.radians, [float(lon1), float(lat1), float(lon2), float(lat2)])\n",
    "    d_lon = lon2 - lon1\n",
    "    d_lat = lat2 - lat1\n",
    "    a = math.sin(d_lat / 2) ** 2 + math.cos(lat1) * math.cos(lat2) * math.sin(d_lon / 2) ** 2\n",
    "    c = 2 * math.asin(math.sqrt(a))\n",
    "    distance = c * EARTH_RADIUS\n",
    "    return distance\n",
    "\n",
    "def clear_list(gjd_list=None, max_speed=56, time_interval=6):\n",
    "    \"\"\"\n",
    "    Remove duplicate timestamps and outlier points\n",
    "    Delete records with duplicate timestamps, keeping the first occurrence\n",
    "    Remove outliers: if the distance/time gap between adjacent points exceeds a threshold, drop the abnormal point\n",
    "    param time_interval: time-splitting threshold; if the gap between adjacent points exceeds this value (here 6 h), split the trajectory into segments\n",
    "    param max_speed: normal-speed threshold; if the average speed between adjacent points exceeds this value (here 56 km/h), remove the abnormal point\n",
    "    param gjd_list: trajectory point list\n",
    "    return: cleaned index list\n",
    "    \"\"\"\n",
    "    if not gjd_list:\n",
    "        return \n",
    "\n",
    "    timestamp_list = []\n",
    "    repeat_timestamp = []\n",
    "    for gjd_index, gjd in enumerate(gjd_list):\n",
    "        gjd_timestamp = gjd[0] \n",
    "        if gjd_timestamp in timestamp_list:\n",
    "            repeat_timestamp.append(gjd_index)  # Log duplicate indices\n",
    "        else:\n",
    "            timestamp_list.append(gjd_timestamp)\n",
    "\n",
    "    valid_indices = [i for i in range(len(gjd_list)) if i not in repeat_timestamp]\n",
    "\n",
    "    # Handle outlier points\n",
    "    final_indices = []  \n",
    "    for idx in valid_indices:\n",
    "        if idx + 1 >= len(gjd_list): \n",
    "            final_indices.append(idx)\n",
    "            break\n",
    "        current_point = gjd_list[idx]  \n",
    "        next_point = gjd_list[idx + 1]  \n",
    "        lon1, lat1, timestamp1 = float(current_point[2]), float(current_point[1]), pd.to_datetime(current_point[0])\n",
    "        lon2, lat2, timestamp2 = float(next_point[2]), float(next_point[1]), pd.to_datetime(next_point[0])\n",
    "        distance = get_distance(lon1, lat1, lon2, lat2)  \n",
    "        time_diff = (timestamp2 - timestamp1).total_seconds() / 3600  \n",
    "\n",
    "        # Check whether a point is an outlier\n",
    "        if time_diff == 0 or (distance / time_diff) > max_speed:\n",
    "            continue  \n",
    "        else:\n",
    "            final_indices.append(idx)\n",
    "    return final_indices\n",
    "\n",
    "target_folder = r\" \"  # Folder path\n",
    "csv_files = [file for file in os.listdir(target_folder) if file.endswith(\".csv\")]\n",
    "\n",
    "for csv_file in csv_files:\n",
    "    file_path = os.path.join(target_folder, csv_file)\n",
    "    try:\n",
    "        data = pd.read_csv(file_path)\n",
    "        gjd_list = data[['BaseDateTime', 'LAT', 'LON']].values.tolist()\n",
    "        valid_indices = clear_list(gjd_list)\n",
    "        cleaned_data = data.iloc[valid_indices]\n",
    "        cleaned_data.to_csv(file_path, index=False)\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing file {csv_file}: {e}\")\n",
    "\n",
    "print(\"All files have been processed.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c8558ae-a31b-4800-901e-b3d7dbe7d258",
   "metadata": {},
   "source": [
    "<span style = 'color:red;font-size:25px'>Extract records with lat/lon outside a specified range"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7dcf44b-5df2-460f-8d17-e3e96093231d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the lat/lon range (convert degrees–minutes–seconds to decimal degrees)\n",
    "def dms_to_decimal(degrees, minutes, seconds):\n",
    "    return degrees + minutes / 60 + seconds / 3600\n",
    "\n",
    "# Latitude range\n",
    "lat_max = dms_to_decimal(27, 37, 19.793)    # 27°37′19.793″\n",
    "lat_min = dms_to_decimal(25, 2, 57.534)     # 25°2′57.534″\n",
    "\n",
    "# Longitude range\n",
    "lon_max = dms_to_decimal(80, 30, 8.39)      # 80°30′8.39″\n",
    "lon_min = dms_to_decimal(78, 16, 39.534)    # 78°16′39.534″\n",
    "target_folder = r\"  \"   # Folder path\n",
    "\n",
    "outside_range_files = []\n",
    "\n",
    "csv_files = [file for file in os.listdir(target_folder) if file.endswith(\".csv\")]\n",
    "\n",
    "# Iterate over each CSV file\n",
    "for csv_file in csv_files:\n",
    "    file_path = os.path.join(target_folder, csv_file)\n",
    "    try:\n",
    "        data = pd.read_csv(file_path, usecols=['LAT', 'LON'])\n",
    "        data = data.dropna(subset=['LAT', 'LON'])\n",
    "        data['LAT'] = pd.to_numeric(data['LAT'], errors='coerce')\n",
    "        data['LON'] = pd.to_numeric(data['LON'], errors='coerce')\n",
    "        data = data.dropna(subset=['LAT', 'LON'])\n",
    "        lat_out_of_range = data[(data['LAT'] < lat_min) | (data['LAT'] > lat_max)]\n",
    "        is_lat_out = not lat_out_of_range.empty\n",
    "        lon_out_of_range = data[(data['LON'] < lon_min) | (data['LON'] > lon_max)]\n",
    "        is_lon_out = not lon_out_of_range.empty\n",
    "\n",
    "        # Record the filename, out-of-range fields, and the exact values as needed\n",
    "        if is_lat_out or is_lon_out:\n",
    "            status = \"\"\n",
    "            details = \"\"\n",
    "            max_display = 5 \n",
    "            if is_lat_out and is_lon_out:\n",
    "                status = \"Latitude and Longitude out of range\"\n",
    "                lat_values = lat_out_of_range['LAT'].values[:max_display]\n",
    "                lon_values = lon_out_of_range['LON'].values[:max_display]\n",
    "                details += f\"Latitude out of range values (showing up to {max_display}):\\n{lat_values}\\n\"\n",
    "                details += f\"Longitude out of range values (showing up to {max_display}):\\n{lon_values}\\n\"\n",
    "            elif is_lat_out:\n",
    "                status = \"Latitude out of range\"\n",
    "                lat_values = lat_out_of_range['LAT'].values[:max_display]\n",
    "                details += f\"Latitude out of range values (showing up to {max_display}):\\n{lat_values}\\n\"\n",
    "            else:\n",
    "                status = \"Longitude out of range\"\n",
    "                lon_values = lon_out_of_range['LON'].values[:max_display]\n",
    "                details += f\"Longitude out of range values (showing up to {max_display}):\\n{lon_values}\\n\"\n",
    "            outside_range_files.append((csv_file, status, details))\n",
    "    except Exception as e:\n",
    "        print(f\"Error reading file {csv_file}: {e}\")\n",
    "\n",
    "# Output all matching CSV filenames, the out-of-range cases, and the corresponding values\n",
    "if outside_range_files:\n",
    "    print(\"The following files contain values outside the specified lat/lon range:\")\n",
    "    for filename, status, details in outside_range_files:\n",
    "        print(f\"\\nFilename: {filename}\")\n",
    "        print(f\"Out-of-range case: {status}\")\n",
    "        print(f\"Out-of-range values:\\n{details}\")\n",
    "else:\n",
    "    print(\"No files were found with values outside the specified lat/lon range.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b54e514f-7ce1-4fbc-906a-eccf11cc3b58",
   "metadata": {},
   "source": [
    "# <span style = 'color:red;font-size:25px'>Interpolate the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26a62659-2d0e-47a1-8744-97648f1b6b5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_folder = r\"   \"   # Folder path\n",
    "\n",
    "csv_files = [file for file in os.listdir(input_folder) if file.endswith(\".csv\")]\n",
    "\n",
    "for csv_file in csv_files:\n",
    "    file_path = os.path.join(input_folder, csv_file)\n",
    "    data = pd.read_csv(file_path)\n",
    "    data = data.drop_duplicates(subset='BaseDateTime', keep='first')\n",
    "    data = data.sort_values(by='BaseDateTime')\n",
    "    data['BaseDateTime'] = pd.to_datetime(data['BaseDateTime'], format='%Y/%m/%d %H:%M:%S')\n",
    "    start_time = data['BaseDateTime'].min()\n",
    "    end_time = data['BaseDateTime'].max()\n",
    "    new_timestamps = pd.date_range(start=start_time, end=end_time, freq='1min')  # 修改为 '1min'\n",
    "    data['Timestamp_numeric'] = data['BaseDateTime'].astype(np.int64) // 10**9\n",
    "    new_timestamps_numeric = new_timestamps.astype(np.int64) // 10**9\n",
    "\n",
    "    # Ensure there are enough data points for interpolation\n",
    "    if len(data) < 2:\n",
    "        continue \n",
    "\n",
    "    # Interpolate the 'Latitude' and 'Longitude' columns\n",
    "    try:\n",
    "        valid_data = data.dropna(subset=['LAT', 'LON'])\n",
    "        if len(valid_data) < 4:\n",
    "            continue  \n",
    "        new_latitudes = []\n",
    "        new_longitudes = []\n",
    "        \n",
    "        for i in range(len(new_timestamps_numeric)):\n",
    "            timestamp = new_timestamps_numeric[i]\n",
    "            prev_idx = np.searchsorted(valid_data['Timestamp_numeric'], timestamp, side='right') - 1\n",
    "            next_idx = prev_idx + 1\n",
    "\n",
    "            # Ensure there are both previous and next points\n",
    "            if prev_idx >= 0 and next_idx < len(valid_data):\n",
    "                prev_timestamp = valid_data.iloc[prev_idx]['Timestamp_numeric']\n",
    "                next_timestamp = valid_data.iloc[next_idx]['Timestamp_numeric']\n",
    "                time_diff = next_timestamp - prev_timestamp\n",
    "                if time_diff <= 120:  \n",
    "                    spline_lat = InterpolatedUnivariateSpline(valid_data['Timestamp_numeric'], valid_data['LAT'])\n",
    "                    spline_lon = InterpolatedUnivariateSpline(valid_data['Timestamp_numeric'], valid_data['LON'])\n",
    "                    new_latitudes.append(spline_lat(timestamp))\n",
    "                    new_longitudes.append(spline_lon(timestamp))\n",
    "                else:  \n",
    "                    lat_interp = np.interp(timestamp, valid_data['Timestamp_numeric'], valid_data['LAT'])\n",
    "                    lon_interp = np.interp(timestamp, valid_data['Timestamp_numeric'], valid_data['LON'])\n",
    "                    new_latitudes.append(lat_interp)\n",
    "                    new_longitudes.append(lon_interp)\n",
    "            else:\n",
    "                new_latitudes.append(np.nan)\n",
    "                new_longitudes.append(np.nan)\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error interpolating 'Latitude' and 'Longitude' in file {csv_file}: {e}\")\n",
    "        continue\n",
    "\n",
    "    # Create a new DataFrame with the interpolated data\n",
    "    interpolated_data = pd.DataFrame({\n",
    "        'BaseDateTime': new_timestamps,\n",
    "        'LAT': new_latitudes,\n",
    "        'LON': new_longitudes\n",
    "    })\n",
    "    interpolated_data['BaseDateTime'] = interpolated_data['BaseDateTime'].apply(lambda x: f\"{x.year}/{x.month}/{x.day} {x.hour}:{x.minute}:{x.second}\")\n",
    "    interpolated_data['Timestamp_numeric'] = pd.to_datetime(interpolated_data['BaseDateTime'], format='%Y/%m/%d %H:%M:%S').astype(np.int64) // 10**9\n",
    "\n",
    "    # Add fixed columns: 'MMSI', 'Ship type', 'Width', 'Length', 'Draught'\n",
    "    fixed_columns = ['MMSI', 'VesselType', 'Width', 'Length', 'Draft']\n",
    "    for col in fixed_columns:\n",
    "        if col in data.columns:\n",
    "            value = data[col].iloc[0] \n",
    "            interpolated_data[col] = value\n",
    "        else:\n",
    "            interpolated_data[col] = np.nan  \n",
    "\n",
    "    # Compute the mean over the previous and next 3–5 values for 'SOG', 'COG', and 'Heading'\n",
    "    for col in ['SOG', 'COG', 'Heading']:\n",
    "        if col in data.columns:\n",
    "            data_col = data[['Timestamp_numeric', col]].dropna()\n",
    "            data_col = data_col.sort_values('Timestamp_numeric').reset_index(drop=True)\n",
    "            averages = []\n",
    "            timestamps_numeric = interpolated_data['Timestamp_numeric'].values\n",
    "            for timestamp in timestamps_numeric:\n",
    "                idx = np.searchsorted(data_col['Timestamp_numeric'], timestamp)\n",
    "                start_idx = max(0, idx - 5)\n",
    "                end_idx = min(len(data_col), idx + 5)\n",
    "                window_data = data_col.iloc[start_idx:end_idx][col]\n",
    "                if len(window_data) >= 1:\n",
    "                    avg_value = window_data.mean()\n",
    "                    averages.append(avg_value)\n",
    "                else:\n",
    "                    averages.append(np.nan)\n",
    "            interpolated_data[col] = averages\n",
    "        else:\n",
    "            interpolated_data[col] = np.nan \n",
    "\n",
    "    interpolated_data.drop(columns=['Timestamp_numeric'], inplace=True)\n",
    "    interpolated_data.to_csv(file_path, index=False)\n",
    "    \n",
    "    print(f\"File {csv_file} has been processed and overwritten.\")\n",
    "\n",
    "print(\"All files have been processed.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "904ef663-f528-4178-bdad-648f52645f69",
   "metadata": {},
   "source": [
    "<span style = 'color:red;font-size:25px'>Median filtering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57ea80f8-e9c1-44ac-8e67-ff05367d7647",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_folder = r\"   \"   # Folder path\n",
    "csv_files = [file for file in os.listdir(input_folder) if file.endswith(\".csv\")]\n",
    "\n",
    "for csv_file in csv_files:\n",
    "    try:\n",
    "        file_path = os.path.join(input_folder, csv_file)\n",
    "        data = pd.read_csv(file_path)\n",
    "        data = data.sort_values(by='BaseDateTime').reset_index(drop=True)\n",
    "        data = data.dropna(subset=['LAT', 'LON']).reset_index(drop=True)\n",
    "        window_size = 5\n",
    "\n",
    "        # Check whether the data length is sufficient to apply filtering\n",
    "        if len(data) >= window_size:\n",
    "            data_to_filter = data[['LAT', 'LON']].copy()\n",
    "            data_filtered_lat = medfilt(data_to_filter['LAT'], kernel_size=window_size)\n",
    "            data_filtered_lon = medfilt(data_to_filter['LON'], kernel_size=window_size)\n",
    "            data.loc[2:, 'LAT'] = data_filtered_lat[2:]\n",
    "            data.loc[2:, 'LON'] = data_filtered_lon[2:]\n",
    "        else:\n",
    "            print(f\"File {csv_file} has fewer data points than the window size; skipping filtering.\")\n",
    "        data.to_csv(file_path, index=False)\n",
    "        print(f\"File {csv_file} has been processed and overwritten.\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing file {csv_file}: {e}\")\n",
    "        continue\n",
    "\n",
    "print(\"All files have been processed.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80894e7a-5c3d-4461-bc83-2e9d0820ba66",
   "metadata": {},
   "source": [
    "<span style = 'color:red;font-size:25px'>Extract points with the same timestamp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9cbdbcac-639e-48fd-b666-95e453bfa1b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_folder = r\"  \"   # Folder path\n",
    "\n",
    "csv_files = [file for file in os.listdir(input_folder) if file.endswith(\".csv\")]\n",
    "all_data = pd.DataFrame()\n",
    "\n",
    "# Read and merge data from each CSV file\n",
    "for csv_file in csv_files:\n",
    "    file_path = os.path.join(input_folder, csv_file)\n",
    "    data = pd.read_csv(file_path)\n",
    "    required_columns = ['# Timestamp', 'MMSI', 'Latitude', 'Longitude', 'SOG', 'COG', 'Heading']\n",
    "    missing_columns = [col for col in required_columns if col not in data.columns]\n",
    "    if missing_columns:\n",
    "        print(f\"File {csv_file} is missing columns {missing_columns}; skipping this file.\")\n",
    "        continue\n",
    "\n",
    "    data['# Timestamp'] = pd.to_datetime(data['# Timestamp'], format='%Y/%m/%d %H:%M:%S', errors='coerce')\n",
    "    data = data.dropna(subset=['# Timestamp'])\n",
    "    data['Timestamp_min'] = data['# Timestamp'].dt.floor('min')  \n",
    "    data = data[['MMSI', 'Timestamp_min', 'Latitude', 'Longitude', 'SOG', 'COG', 'Heading']]\n",
    "    all_data = pd.concat([all_data, data], ignore_index=True)\n",
    "\n",
    "all_data = all_data.sort_values(by='Timestamp_min')\n",
    "unique_timestamps = all_data['Timestamp_min'].drop_duplicates().reset_index(drop=True)\n",
    "ship_order = all_data.drop_duplicates(subset='MMSI', keep='first').sort_values('Timestamp_min')['MMSI'].tolist()\n",
    "result_df = pd.DataFrame(index=unique_timestamps)\n",
    "\n",
    "# Iterate over each vessel, build column names, and fill the data\n",
    "for mmsi in ship_order:\n",
    "    ship_data = all_data[all_data['MMSI'] == mmsi]\n",
    "    ship_data = ship_data.set_index('Timestamp_min')\n",
    "    ship_data = ship_data[['Latitude', 'Longitude', 'SOG', 'COG', 'Heading']]\n",
    "    ship_columns = [f\"{mmsi}_Latitude\", f\"{mmsi}_Longitude\", f\"{mmsi}_SOG\", f\"{mmsi}_COG\", f\"{mmsi}_Heading\"]\n",
    "    ship_data.columns = ship_columns\n",
    "    result_df = result_df.join(ship_data, how='left')\n",
    "\n",
    "result_df = result_df.reset_index(drop=False)  \n",
    "result_df.insert(0, 'Timestamp_str', result_df['Timestamp_min'].dt.strftime('%Y/%m/%d %H:%M:%S'))\n",
    "final_data = result_df.drop(columns=['Timestamp_min'])\n",
    "\n",
    "# Insert two rows at the top of the result DataFrame: MMSI and column names\n",
    "mmsi_row = ['']  \n",
    "for mmsi in ship_order:\n",
    "    mmsi_row.extend([mmsi]*5)  \n",
    "    \n",
    "columns_row = ['# Timestamp']  \n",
    "for _ in ship_order:\n",
    "    columns_row.extend(['Latitude', 'Longitude', 'SOG', 'COG', 'Heading'])\n",
    "final_df = pd.DataFrame([mmsi_row, columns_row], columns=final_data.columns)\n",
    "final_df = pd.concat([final_df, final_data], ignore_index=True)\n",
    "output_file = \"   \" # Output file path\n",
    "final_df.to_csv(output_file, index=False, header=False, encoding='utf-8-sig')\n",
    "\n",
    "print(f\"Data processing complete. Results saved to {output_file}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5f05fa8-8c5b-45d2-ad15-925737a58d6d",
   "metadata": {},
   "source": [
    "<span style = 'color:red;font-size:25px'>Extract static attribute values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e9277d3-abeb-45ec-9947-757b2f57db60",
   "metadata": {},
   "outputs": [],
   "source": [
    "summary_file = r\"   \"   # Summary output file path\n",
    "input_folder = r\"   \"   # Raw data folder path\n",
    "output_file = r\"  \"     # Output file path\n",
    "\n",
    "summary_data = pd.read_csv(summary_file, header=None, low_memory=False)\n",
    "mmsi_row = summary_data.iloc[0]\n",
    "columns_row = summary_data.iloc[1]\n",
    "\n",
    "# Get the MMSI order (drop missing values and duplicates)\n",
    "mmsi_list = []\n",
    "for mmsi_value in mmsi_row:\n",
    "    if pd.notna(mmsi_value) and mmsi_value != '':\n",
    "        try:\n",
    "            mmsi = str(int(float(mmsi_value)))\n",
    "            if mmsi not in mmsi_list:\n",
    "                mmsi_list.append(mmsi)\n",
    "        except ValueError:\n",
    "            continue  \n",
    "\n",
    "# Initialize the set of processed MMSIs\n",
    "processed_mmsi = set()\n",
    "mmsi_attributes = {}\n",
    "\n",
    "# Iterate over the MMSI list and extract attribute values\n",
    "for mmsi in mmsi_list:\n",
    "    if mmsi in processed_mmsi:\n",
    "        continue  \n",
    "    found = False\n",
    "    for file_name in os.listdir(input_folder):\n",
    "        if file_name.endswith('.csv'):\n",
    "            file_path = os.path.join(input_folder, file_name)\n",
    "            try:\n",
    "                data = pd.read_csv(file_path)\n",
    "                if 'MMSI' in data.columns and mmsi in data['MMSI'].astype(str).unique():\n",
    "                    ship_data = data[data['MMSI'].astype(str) == mmsi]\n",
    "                    ship_type = ship_data['Ship type'].iloc[0] if 'Ship type' in ship_data.columns else ''\n",
    "                    width = ship_data['Width'].iloc[0] if 'Width' in ship_data.columns else ''\n",
    "                    length = ship_data['Length'].iloc[0] if 'Length' in ship_data.columns else ''\n",
    "                    draught = ship_data['Draught'].iloc[0] if 'Draught' in ship_data.columns else ''\n",
    "                    mmsi_attributes[mmsi] = [ship_type, width, length, draught]\n",
    "                    found = True\n",
    "                    break  \n",
    "            except Exception as e:\n",
    "                print(f\"Error reading file {file_name}: {e}\")\n",
    "                continue\n",
    "    if not found:\n",
    "        print(f\"Data for MMSI {mmsi} not found; attribute values will be left empty.\")\n",
    "        mmsi_attributes[mmsi] = ['', '', '', '']\n",
    "    processed_mmsi.add(mmsi)\n",
    "\n",
    "# Build the result DataFrame\n",
    "# Row 1: MMSI\n",
    "first_row = []\n",
    "for mmsi in mmsi_list:\n",
    "    first_row.extend([mmsi] * 4) \n",
    "\n",
    "# Row 2: attribute column names\n",
    "second_row = []\n",
    "for _ in mmsi_list:\n",
    "    second_row.extend(['Ship type', 'Width', 'Length', 'Draught'])\n",
    "\n",
    "# Row 3: attribute values\n",
    "third_row = []\n",
    "for mmsi in mmsi_list:\n",
    "    attributes = mmsi_attributes.get(mmsi, ['', '', '', ''])\n",
    "    third_row.extend(attributes)\n",
    "\n",
    "# Check whether lengths match\n",
    "print(f\"first_row length: {len(first_row)}\")\n",
    "print(f\"second_row length: {len(second_row)}\")\n",
    "print(f\"third_row length: {len(third_row)}\")\n",
    "\n",
    "# Create the DataFrame\n",
    "num_columns = len(first_row)\n",
    "result_df = pd.DataFrame(columns=range(num_columns))\n",
    "result_df.loc[0] = first_row\n",
    "result_df.loc[1] = second_row\n",
    "result_df.loc[2] = third_row\n",
    "\n",
    "result_df.to_csv(output_file, index=False, header=False, encoding='utf-8-sig')\n",
    "print(f\"Vessel attributes have been extracted and saved to {output_file}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
